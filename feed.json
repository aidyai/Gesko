{
    "version": "https://jsonfeed.org/version/1",
    "title": "Gesko",
    "home_page_url": "https://aidyai.github.io//Gesko/",
    "feed_url": "https://aidyai.github.io//Gesko/feed.json",
    "description": "just a minimalist, personal blog",
    "icon": "https://aidyai.github.io//Gesko/apple-touch-icon.png",
    "favicon": "https://aidyai.github.io//Gesko/favicon.ico",
    "expired": false,
    
    "author":  {
        "name": "gesko",
        "url": null,
        "avatar": null
    },
    
"items": [
    
        {
            "id": "https://aidyai.github.io//Gesko/2022/12/20/understanding-diffusion-models",
            "title": "Understanding diffusion models",
            "summary": null,
            "content_text": "UNDERSTANDING DIFFUSION MODELS  __DIFFUSION MODELS + DENOISING DIFFUSION MODEL EXPLAINED  __3 POPULAR SOTA DIFFUSION MODELS  __SOURCE CODE FOR IMPLEMENTATION  __CONCLUSION  __REFERENCESIt is no news that Diffusion models are the best of the best Generative models that have surpassed GANS in Computer Vision and other Multi-Modal Learning tasks. These models have created significant feats in arts, generative NFT’s, Interior design, personalized avatar creations, 3d modelling, urban design etc.This post is going to be a two part series where I first of all expose how Denoising Diffusion models work which is the foundation upon which all these SOTA models (GLIDE, DALLE.2, SD) are being built.DIFFUSION MODELSDIffusion model are a class of state-of-the art deep generative models that have shown impressive results on various tasks ranging from multi-modal modelling Vision to NLP to waveform signal Processing even to 3d object generation (but not comparable to humans). These models have achieved very impressive quality and diversity of sample synthesis than other state-of-the art genrative models like GANS, VAEs, Normalizing Flows etc.            Diffusion model are generative models meaning it is a model used to generate data similar to the data which it was trained on. A generative model attempts to learn the data distribution over $p(x      y)$ where $x$ is the image and $y$ the labels in order to generate novel samples. Onced trained, a generative model can generate novel samples from an approximation of $p(x      y)$, denoted $p_\\theta({x      y})$      Areas Where Diffusion Models have been applied to  Image Super Resolution  Image Inpainting  Image Outpainting  Image to Image generation  Semantic Segmentation  Point Cloud Completion and Generation  Text-to-Image Generation  Text-to-Audio GenerationSTATE OF THE ART DIFFUSION MODELS (cherrypicked)  DALLE 2  STABLE DIFFUSIONUNDERSTANDING THE FOUNDATIONS (DDPMs)Originally introduced by Sohl-Dickstein et al. (2015) in his paper “Deep unsupervised learning using nonequilibrium thermodynamics” which was followed up by Ho et al. (2020) in his paper “Denoising diffusion probabilistic models” (DDPMs), as well every other paper that has built on it. It is a latent variable generative model inspired by non-equilibrum thermodynamics in physics where samples generated is made possible by denoising process.  Denoising Diffusion models is a two step model consisting of a forward process also called noising process and a backward process also know called denoisng or reverse noising process.  It works by adding gaussian noise to input data which could be an image following a Markov Chain.  The forward process follows a Markov chain where a little bit of gaussian noise is added to the input image which progressively disturbs the data distribution untill it is completely destroyed or unrecognised from gaussian noise.  The backward or reverse process learns to restore the data to its original form.  Reversing or removing the noise means recovering the values of the pixels, so that the resulting image will be similar to the original image. The reverse diffusion process takes a noisy image and learns to generate a less noisy version of that image, this process will be repeated until noise is converted to data. OR  In order to generate new data, standard gaussian noise is used to perform the  denoising. This process will be repeated until noise is converted to data.  The recovering or noise reversal is parameterized because it uses a neural network. The task of the Neural Network is to predict the noise that was added in a given image.  The objective function of this model was simplified to a MSE Loss i.e given a noised image, the noise added is predicted and then this predicted noise is subtracted from the noise to get the real image. This is basically what is happenning when training a Diffusion Model, It is learning to denoise.  In DDPMs the forward process is fixed while the reverse process is what needs learning meaning we need to train only a single neural network.  The important components of a denoising diffusion mdoels include:          UNET Architecture      positional emebedidng      noise scheduler      attention mechanism      MATHEMATICAL EXPLANATIONA. NOISING:DEFINING THE PROCESS FORMALLYFORWARD PROCESS:The forward diffusion process starts from data (image) and generates this intermediate noisy images by simply adding noise one step at a timeAt every step, a normal distribution will be used to generate an image conditioned on the previous image.            the normal distribution which is represented as $q(x_t      x_{t-1}) = \\mathcal{N(x_t; \\sqrt{1-\\beta_t{x_{t-1}}\\beta_t}I)}$ is going to take $x_{t-1}$ the prevoius step and generate $x_t$ the current step. It takes $x_0$ and it generates $x_1$      A normal distribution over the current step $x_t$ where the mean is $(\\mathcal{\\sqrt{1-\\beta_t}})$ times the image at the prevoius time step which is ${x_{t-1}}$ and ${\\beta_t}I$ represents the variance scheduler which in the real sense is a very small positive scalar value $0.001$This normal distribution, $\\mathcal{N(x_t; {\\sqrt{1-\\beta_t}{x_{t-1}}, \\beta_t}I)}$  takes the image at the previous step, rescales the pixel values in this image and then adds tiny bit of noise via the variance scheduler “per time step”JOINT DISTRIBUTION:A joint distibution can also be defined for all the samples generated in the forward process starting from $x_1$ all the way to $x_T$.  The joint distribution which is the samples conditioned on $x_0$ is the cumulateive product of the conditionals that are formed at each step as such $q(x_1,…,x_T|x_0)$ defines the joint distribution of all the samples that will be generated in the forward markov process\\(q(x_1,...,x_T|x_0) = \\prod^T_{t=1}   {q(x_t|x_{t-1})}\\)Speed?Why can’t we use $x_0$ our input image to generate noisy samples at any time step say $x_{10}$. Simply put can’t we use $x_0$ to generate $x_{10}$ ?. We can do that by making${\\alpha}_t$ = $1-\\beta_t$ , then $\\bar{\\alpha}_t$ which is the cumulative product of ${\\alpha}_t$ now becomes\\(\\bar{\\alpha}_t = \\prod^{t}_{s=1}(1-\\beta_s)\\)In order to answer the speed question we can then rewrite the original formular as follows:\t\t$q(x_t|x_0) = \\mathcal{N(x_t;\\sqrt{\\bar{\\alpha_t}}x_0,(1-{\\bar{\\alpha_t}})I)}$Using the reparameterization trick we can sample $x_t$ as follows$x_t$ = $\\sqrt{\\bar{\\alpha}}\\space x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\space \\epsilon$  where $\\epsilon \\sim{\\mathcal{N(0,1)}}$  and ${1-\\bar{\\alpha_t}}$ is our noise schedule at any time step, as such given $x_0$ we can draw samples at any time step $t$.It should also be noted that the forward diffusion process is defined such that as  $(x_T\\mid{x_0})$ approaches infinity it becomes indistinguishable from standard normal distribution $\\mathcal{N({x_T;(0,1)})}$. ___  The forward chain pertubs the data distribution by gradually adding Gaussian noise to the ground truth image with a pre-designed schedule until the data distribution converges to a given prior, i.e., a standard Gaussian Distribution – (Isometric Gaussian).\\(q(x_1,...,x_T|x_0) = \\prod^T_{t=1}   {q(x_t|x_{t-1})} ---(1)\\)\\(q(x_t|x_{t-1}) = \\mathcal{N(x_t; \\sqrt{1-\\beta_t{x_{t-1}}\\beta_t}I)}   ---(2)\\) $q(x_t)$ is used to denote the distributions of latent variables $x_t$ in the forward process.The noising process defined in Eq.(2) allows us to sample an arbitrary step of the noised latents directly conditioned on the input $x_o$.\tWhere $\\alpha_t$ = $1-\\beta_t$ and $\\bar{\\alpha_t}$ = $\\prod^t_{s=0}$ $\\bar{\\alpha_s}$, we can wite the marginal as:\t\\(q(x_t|x_0) = \\mathcal{N(x_t;\\sqrt{\\bar{\\alpha_t}}x_0,(1-{\\bar{\\alpha_t}})I)}\\)\\(x_t = \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-{\\bar{\\alpha_t}}}\\epsilon\\)When $\\bar{\\alpha}_t$ approximates 0, $x_T$ is practically indistinguisahble from pure Gaussian noise: $p(x_T)\\approx$ $\\mathcal{N}(x_T;0,1)$.B. DENOISING:  What is Means to Reverse The Noise?DENOISING: DEFINING THE GENERATIVE MODEL BY DENOISINGIn order to generate data from a diffusion model, we will start from pure noise which is a standard normal distribution with zero mean and unit variance and generates data by denoising one step at a time.As such $p(x_T)$ = $\\mathcal{N(x_T;(0,1))}$  is the distribution of data at the end of the forward diffusion process.the parametric denoising distribution can be defined as follows $p_\\theta(x_{t-1}\\mid{x_t})$ = $\\mathcal{N({x_{t-1};{\\mu}\\theta{(x_t,t)},{\\Sigma\\theta({x_t,{t}}})})}$ apart from the sample $x_t$ at time $t$ the model also takes $t$ as input in order to account for the different noise levels at different time steps in the forward process noise schedule so that the model can learn to undo this individuallyJoint DistributionThe joint distribution can be written as It is the product of the base distibution $p{(x_T)}$ and the product of the conditionals which still follows a markov process\t$x_0\\Leftarrow \\cdots  \\Leftarrow \\cdots  \\Leftarrow x_{T-1}\\cdots  \\Leftarrow x_T$\\[p_\\theta(x_{0:T})=p(x_T)\\prod^T_{t=1}p_\\theta(x_{t-1}|x_t)\\]The model is now tasked with learning the probability density of an earlier time step given the current timestep. During this process iteration is done from pure noise $x_T$ to $x_0$ our final imageStarting from sampled noise, the diffusion model performs $T$ denoising steps until a sharp image is formed.The denosing process produces a series of intermediate images with decreasing levels of noise, denote as $x_T, x_{T-1},…x_0$,Given only $x_T$ which is indistinguishable from gaussian noise we can get $x_0$ an output image.The reverse process takes the completely noised image and learns to gradually revert the Markov chain of noise corruption to the ground truth. The reversed process is then written as follows: \\(p_\\theta({x_{t-1}|x_t}) = \\mathcal{N}(x_{t-1};\\mu(x_t,t, {\\Sigma}_\\theta(x_t,t))\\)__In Denoising Diffusion Models,  The Noise $\\epsilon$ is what is Predicted and this is done by Optimizing the variational upper bound on the negative log-likelihood.__\\(\\\\E\\ [-\\log{p_{\\theta}}(x_0)]&lt; \\\\ E_q[-\\log\\frac{p_\\theta({x_{0:T}})}{q(x_{1:T}|x_0)}]--(7)\\)\\(\\\\ E_q[-\\log{p{(x_T)}}-\\sum_{t&gt;=1}{\\log\\frac{p_\\theta({x_{0:T}})}{q(x_{1:T}|x_0)}} ]-- (8)\\)\\(= -L_{VLB}\\)__Reparametrization have been appplied to Eq. (8), which results in the general objective below:\\(E_{t\\sim{\\mathcal{U(0,T),x_0{\\sim{q{(x_0),\\epsilon\\sim{\\mathcal{N(0,1)}}}}}}}}[\\lambda{(t)}||\\epsilon-{\\epsilon_\\theta}(x_t,t)||^2]   --(10)\\)__The neural network ${\\epsilon_\\theta}(x_t,t)$ predicts $\\epsilon$ by minimizing the loss = ${||\\epsilon-\\epsilon_\\theta}(x_t,t)||^2$ which is the $L_2$ Loss \tINTUITIVELY: Given a noised image, the noise added is predicted and then this predicted noise is subtracted from the noise to get the real image. This is basically what is happenning when training a Diffusion Model, It is learning to denoise.Both the forward Diffusion processes $q(x_t|x_{t-1})$ and the backward or reconstruction process $q(x_{t-1}|x_t)$ are modelled as the products of Markov transition probabilities:\\(q(x_{0:T}) = q(x_0)\\prod_{t=1}^T{q(x_t|x_{t-1})}, p_\\theta(x_{T:0}) = p(x_{T})\\prod_{t=T}^1{p_{\\theta}(x_{t-1}|x_t)},\\)$q(x_0)$ is the real data distributionIMPORTantDiffusion models are latent variable models\tLatent variables$:$ = $x_1,x_{2},x_3,x_4,\\cdots x_T$\tObserved variables$:$ $x_0$TRAINING A DENOSING DIFFUSION PROBABILISTIC MODELThe reverse step process is only tasked with learning the mean while its variance is set to a constant__OBJECTIVE FUNCTION OF A DDPM\\(E_{x_0{\\sim{q{(x_0),\\epsilon\\sim{\\mathcal{N(0,1)t\\sim{\\mathcal{U(0,T),}}}}}}}}[||\\epsilon-{\\epsilon_\\theta}(\\sqrt{\\bar{\\alpha}}\\space x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\space \\epsilon),t||^2]\\)\tWhere $x_t = \\sqrt{\\bar{\\alpha}}\\space x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\space \\epsilon)$                     $E_{x_0{\\sim{q{(x_0),\\epsilon\\sim{\\mathcal{N(0,1)t\\sim{\\mathcal{U(0,T),}}}}}}}}[||\\epsilon-{\\epsilon_\\theta}(x_t,t)||^2]$Our loss funtion finally looks like this$:$\t\t\t                $L_{simple}={E_{x_0,t,\\epsilon}}[||\\epsilon_-\\epsilon_{\\theta}(x_t,t)||^2]$____The Training Algorithm looks like this: ![[trainDIFF.PNG]]IMAGE GENERATION or SAMLING FROM A DENOSING DIFFUSION PROBABILISTIC MODELThe Sampling Algorithm looks like this: ![[sampleDIFF.PNG]]IMPORTANT COMPONENTS OF A DENOISING DIFFUSION MODEL      UNET ARCHITECTURE: The UNET Architecture is a Convolutional Network originally developed for biomedical image segmentation. It has a U Shape and it does downsampling on one part and upsampling on the other side and all its operation are convolution based.        POSITIONAL EMBEDDING: Positional embedding as originally used in the Attention is all you need paper was utilized for train the neural network has shared parameters across time which means it can’t distinguish between various timesteps as such it needs to filter the noise across images with different noise intensities.    PE is added as a way of encoding positional information into the UNET Model to help distinguish the various noise intensities within the markov chain process    The positional embedding is added as additional information in the downsample, middle and upsample block of the UNET Model.    PE are wave frequencies used to capture positonal information at both odd $sin()$ and even $cosine()$ positions and this embeddings can be calculated as follows.    $PE({pos,2i}) = sin(pos\\div{1000^{2i/d}})$ $PE({pos,2i+1}) = cos(pos\\div{1000^{2i/d}})$        NOISE SCHEDULER: This is just a technique that makes possible an iterative addition of noise to an image of adding noise.        ATTENTION MECHANISM: It is a mechanism that makes a model selectively focus on the varying parts of the inputs. It assigns weights to the different positions which indicates the importance of the input sequence for generating outputs by calculating attention scores. Originally in the transformers it was used both in the encoder and decoder parts of the Model.  IMPLEMENTING DENOISNG DIFFUSION MODEL WITH PYTORCHIt should be noted that I will not run training because it is needless and basically useless to train this kind of model because it is not state of the art as compared to the other models briefly explained above except you have a gpu compute where you can run it for over 1000 epochs. In part 2 of this article you will see a detailed handson code for finetuning stable diffusion using dreambooth.import torchfrom torch import nnfrom torch.nn import functional as Fimport numpy as npimport matplotlib.pyplot as pltimport torchvisionfrom torchvision import transformsfrom torch.utils.data import DataLoaderfrom tqdm.notebook import tqdmfrom torch.utils.data import Subsetimport copydef show_images(images, title=\"\"):    \"\"\"Shows the provided images as sub-pictures in a square\"\"\"    images = [np.clip(im.permute(1,2,0).numpy(),0,1) for im in images]      # Defining number of rows and columns    fig = plt.figure(figsize=(8, 8))    rows = int(len(images) ** (1 / 2))    cols = round(len(images) / rows)     # Populating figure with sub-plots    idx = 0    for r in range(rows):        for c in range(cols):            fig.add_subplot(rows, cols, idx + 1)            if idx &lt; len(images):                plt.imshow(images[idx])                plt.axis('off')                idx += 1    fig.suptitle(title, fontsize=30)    # Showing the figure    plt.show()device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")devicedef sinusoidal_embedding(n, d):    # Returns the standard positional embedding    embedding = torch.tensor([[i / 10_000 ** (2 * j / d) for j in range(d)] for i in range(n)])    sin_mask = torch.arange(0, n, 2)    embedding[sin_mask] = torch.sin(embedding[sin_mask])    embedding[1 - sin_mask] = torch.cos(embedding[sin_mask])    return embeddingclass double_conv(nn.Module):    def __init__(self, in_ch, out_ch):        super(double_conv, self).__init__()        self.conv = nn.Sequential(            nn.Conv2d(in_ch, out_ch, 3, padding=1),            nn.BatchNorm2d(out_ch),            nn.ReLU(inplace=True),            nn.Conv2d(out_ch, out_ch, 3, padding=1),            nn.BatchNorm2d(out_ch),            nn.ReLU(inplace=True)        )      def forward(self, x):        x = self.conv(x)        return xclass down_layer(nn.Module):    def __init__(self, in_ch, out_ch):        super(down_layer, self).__init__()        self.pool = nn.MaxPool2d(2, stride=2, padding=0)        self.conv = double_conv(in_ch, out_ch)    def forward(self, x):        x = self.conv(self.pool(x))        return xclass up(nn.Module):    def __init__(self, in_ch, out_ch):        super(up, self).__init__()        self.up_scale = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)      def forward(self, x1, x2): # x1 (bs,out_ch,w1,h1) x2 (bs,in_ch,w2,h2)        x2 = self.up_scale(x2) # (bs,out_ch,2*w2,2*h2)        diffY = x1.size()[2] - x2.size()[2]        diffX = x1.size()[3] - x2.size()[3]        x2 = F.pad(x2, [diffX // 2, diffX - diffX // 2,                        diffY // 2, diffY - diffY // 2]) # (bs,out_ch,w1,h1)        x = torch.cat([x2, x1], dim=1) # (bs,2*out_ch,w1,h1)        return xclass up_layer(nn.Module):    def __init__(self, in_ch, out_ch): # !! 2*out_ch = in_ch !!        super(up_layer, self).__init__()        self.up = up(in_ch, out_ch)        self.conv = double_conv(in_ch, out_ch)      def forward(self, x1, x2): # x1 (bs,out_ch,w1,h1) x2 (bs,in_ch,w2,h2)        a = self.up(x1, x2) # (bs,2*out_ch,w1,h1)        x = self.conv(a) # (bs,out_ch,w1,h1) because 2*out_ch = in_ch        return xclass UNet(nn.Module):    def __init__(self, in_channels=1, n_steps=1000, time_emb_dim=100):        super(UNet, self).__init__()        self.conv1 = double_conv(in_channels, 64)        self.down1 = down_layer(64, 128)        self.down2 = down_layer(128, 256)        self.down3 = down_layer(256, 512)        self.down4 = down_layer(512, 1024)        self.up1 = up_layer(1024, 512)        self.up2 = up_layer(512, 256)        self.up3 = up_layer(256, 128)        self.up4 = up_layer(128, 64)        self.last_conv = nn.Conv2d(64, in_channels, 1)        # Time embedding        self.time_embed = nn.Embedding(n_steps, time_emb_dim)        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)        self.time_embed.requires_grad_(False)        self.te1 = self._make_te(time_emb_dim, in_channels)        self.te2 = self._make_te(time_emb_dim, 64)        self.te3 = self._make_te(time_emb_dim, 128)        self.te4 = self._make_te(time_emb_dim, 256)        self.te5 = self._make_te(time_emb_dim, 512)        self.te1_up = self._make_te(time_emb_dim, 1024)        self.te2_up = self._make_te(time_emb_dim, 512)        self.te3_up = self._make_te(time_emb_dim, 256)        self.te4_up = self._make_te(time_emb_dim, 128)      def _make_te(self, dim_in, dim_out):        return nn.Sequential(nn.Linear(dim_in, dim_out), nn.SiLU(), nn.Linear(dim_out, dim_out))    def forward(self, x , t): # x (bs,in_channels,w,d)        bs = x.shape[0]        t = self.time_embed(t)        x1 = self.conv1(x+self.te1(t).reshape(bs, -1, 1, 1)) # (bs,64,w,d)        x2 = self.down1(x1+self.te2(t).reshape(bs, -1, 1, 1)) # (bs,128,w/2,d/2)        x3 = self.down2(x2+self.te3(t).reshape(bs, -1, 1, 1)) # (bs,256,w/4,d/4)        x4 = self.down3(x3+self.te4(t).reshape(bs, -1, 1, 1)) # (bs,512,w/8,h/8)        x5 = self.down4(x4+self.te5(t).reshape(bs, -1, 1, 1)) # (bs,1024,w/16,h/16)        x1_up = self.up1(x4, x5+self.te1_up(t).reshape(bs, -1, 1, 1)) # (bs,512,w/8,h/8)        x2_up = self.up2(x3, x1_up+self.te2_up(t).reshape(bs, -1, 1, 1)) # (bs,256,w/4,h/4)        x3_up = self.up3(x2, x2_up+self.te3_up(t).reshape(bs, -1, 1, 1)) # (bs,128,w/2,h/2)        x4_up = self.up4(x1, x3_up+self.te4_up(t).reshape(bs, -1, 1, 1)) # (bs,64,w,h)        output = self.last_conv(x4_up) # (bs,in_channels,w,h)        return outputbs = 3x = torch.randn(bs,1,32,32)n_steps=1000timesteps = torch.randint(0, n_steps, (bs,)).long()unet = UNet()y = unet(x,timesteps)y.shapeclass DDPM(nn.Module):    def __init__(self, network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device) -&gt; None:        super(DDPM, self).__init__()        self.num_timesteps = num_timesteps        self.betas = torch.linspace(beta_start, beta_end, num_timesteps, dtype=torch.float32).to(device)        self.alphas = 1.0 - self.betas        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)        self.network = network        self.device = device        self.sqrt_alphas_cumprod = self.alphas_cumprod ** 0.5 # used in add_noise        self.sqrt_one_minus_alphas_cumprod = (1 - self.alphas_cumprod) ** 0.5 # used in add_noise and step      def add_noise(self, x_start, x_noise, timesteps):        # The forward process        # x_start and x_noise (bs, n_c, w, d)        # timesteps (bs)        s1 = self.sqrt_alphas_cumprod[timesteps] # bs        s2 = self.sqrt_one_minus_alphas_cumprod[timesteps] # bs        s1 = s1.reshape(-1,1,1,1) # (bs, 1, 1, 1) for broadcasting        s2 = s2.reshape(-1,1,1,1) # (bs, 1, 1, 1)        return s1 * x_start + s2 * x_noise    def reverse(self, x, t):        # The network return the estimation of the noise we added        return self.network(x, t)            def step(self, model_output, timestep, sample):        # one step of sampling        # timestep (1)        t = timestep        coef_epsilon = (1-self.alphas)/self.sqrt_one_minus_alphas_cumprod        coef_eps_t = coef_epsilon[t].reshape(-1,1,1,1)        coef_first = 1/self.alphas ** 0.5        coef_first_t = coef_first[t].reshape(-1,1,1,1)        pred_prev_sample = coef_first_t*(sample-coef_eps_t*model_output)        variance = 0        if t &gt; 0:            noise = torch.randn_like(model_output).to(self.device)            variance = ((self.betas[t] ** 0.5) * noise)        pred_prev_sample = pred_prev_sample + variance        return pred_prev_sampledef training_loop(model, dataloader, optimizer, num_epochs, num_timesteps, device=device):    \"\"\"Training loop for DDPM\"\"\"    global_step = 0    losses = []    for epoch in range(num_epochs):        model.train()        progress_bar = tqdm(total=len(dataloader))        progress_bar.set_description(f\"Epoch {epoch}\")        for step, batch in enumerate(dataloader):            batch = batch[0].to(device)            noise = torch.randn(batch.shape).to(device)            timesteps = torch.randint(0, num_timesteps, (batch.shape[0],)).long().to(device)              noisy = model.add_noise(batch, noise, timesteps)            noise_pred = model.reverse(noisy, timesteps)            loss = F.mse_loss(noise_pred, noise)            optimizer.zero_grad()            loss.backward()            optimizer.step()            progress_bar.update(1)            logs = {\"loss\": loss.detach().item(), \"step\": global_step}            losses.append(loss.detach().item())            progress_bar.set_postfix(**logs)            global_step += 1        progress_bar.close()root_dir = './'transforms01 = torchvision.transforms.Compose([        torchvision.transforms.ToTensor(),        #torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))    ])dataset = torchvision.datasets.CIFAR10(root=root_dir, train=True, transform=transforms01, download=True)dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=512, shuffle=True,num_workers=10)for b in dataloader:    batch = b[0]    break    bn = [b for b in batch[:100]]show_images(bn, \"origin\")![[res.png]]learning_rate = 1e-3num_epochs = 15num_timesteps = 1000network = UNet(in_channels=3)network.to(device)model = DDPM(network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device)model.train()optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)training_loop(model, dataloader, optimizer, num_epochs, num_timesteps, device=device)Model will start training for about 15 epochsdef generate_image(ddpm, sample_size, channel, size):    \"\"\"Generate the image from the Gaussian noise\"\"\"      frames = []    frames_mid = []    ddpm.eval()    with torch.no_grad():        timesteps = list(range(ddpm.num_timesteps))[::-1]        sample = torch.randn(sample_size, channel, size, size).to(device)        for i, t in enumerate(tqdm(timesteps)):            time_tensor = (torch.ones(sample_size) * t).long().to(device)            residual = ddpm.reverse(sample, time_tensor).to(device)            sample = ddpm.step(residual, time_tensor[0], sample)            if t==500:                #sample_squeezed = torch.squeeze(sample)                for i in range(sample_size):                    frames_mid.append(sample[i].detach().cpu())        #sample = torch.squeeze(sample)        for i in range(sample_size):            frames.append(sample[i].detach().cpu())    return frames, frames_middef make_dataloader(dataset, class_name ='ship'):    s_indices = []    s_idx = dataset.class_to_idx[class_name]    for i in range(len(dataset)):        current_class = dataset[i][1]        if current_class == s_idx:            s_indices.append(i)    s_dataset = Subset(dataset, s_indices)    return torch.utils.data.DataLoader(dataset=s_dataset, batch_size=512, shuffle=True)ship_dataloader = make_dataloader(dataset)ship_network = copy.deepcopy(network)ship_model = DDPM(ship_network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device)num_epochs = 10num_timesteps = model.num_timestepslearning_rate = 1e-3ship_model.train()optimizer = torch.optim.Adam(ship_model.parameters(), lr=learning_rate)training_loop(ship_model, ship_dataloader, optimizer, num_epochs, num_timesteps, device=device)generated, generated_mid = generate_image(ship_model, 100, 3, 32)show_images(generated, \"Generated ships\")![[ddpm.png]]This training was only done for very few epochs thats why we still do not have a very detailed generated result.CONCLUSIONSThis article covered the foundations of Diffusion model by going deeply into Denoising diffusion models, the concepts, the maths and the code. There’s actually no point training this kind of an AI model because it wont result in any generation that is as good as DALL-E or Stable Diffusion in terms of Fidelity and Diveristy. As such we have SOTA Models that can perform text to image generation better than this one even though the purpose of this article was just to give an overview of the building blocks of a diffusion model.__REFERENCES  Karsten Kreis; Ruigui Gao; Arash Vahdat (2022-5-4): “Denoising Diffusion-based Generative Modelling: Foundations and Applications (CVPR 2002 Worskshop) “  Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models, 2020.  Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics, 2015.  https://github.com/dataflowr/notebooks/blob/master/Module18/ddpm_micro_sol.ipynb  Karsten Kreis; Ruigui Gao; Arash Vahdat (2022-5-4): “Denoising Diffusion-based Generative Modelling: Foundations and Applications (CVPR 2002 Worskshop) “",
            "content_html": "<h1 id=\"understanding-diffusion-models\">UNDERSTANDING DIFFUSION MODELS</h1><ol>  <li>__DIFFUSION MODELS + DENOISING DIFFUSION MODEL EXPLAINED</li>  <li>__3 POPULAR SOTA DIFFUSION MODELS</li>  <li>__SOURCE CODE FOR IMPLEMENTATION</li>  <li>__CONCLUSION</li>  <li>__REFERENCES</li></ol><p>It is no news that Diffusion models are the best of the best Generative models that have surpassed GANS in Computer Vision and other Multi-Modal Learning tasks. These models have created significant feats in arts, generative NFT’s, Interior design, personalized avatar creations, 3d modelling, urban design etc.</p><p>This post is going to be a two part series where I first of all expose how Denoising Diffusion models work which is the foundation upon which all these SOTA models (GLIDE, DALLE.2, SD) are being built.</p><h3 id=\"diffusion-models\">DIFFUSION MODELS</h3><p>DIffusion model are a class of state-of-the art deep generative models that have shown impressive results on various tasks ranging from multi-modal modelling Vision to NLP to waveform signal Processing even to 3d object generation (but not comparable to humans). These models have achieved very impressive quality and diversity of sample synthesis than other state-of-the art genrative models like GANS, VAEs, Normalizing Flows etc.</p><table>  <tbody>    <tr>      <td>Diffusion model are generative models meaning it is a model used to generate data similar to the data which it was trained on. A generative model attempts to learn the data distribution over $p(x</td>      <td>y)$ where $x$ is the image and $y$ the labels in order to generate novel samples. Onced trained, a generative model can generate novel samples from an approximation of $p(x</td>      <td>y)$, denoted $p_\\theta({x</td>      <td>y})$</td>    </tr>  </tbody></table><h3 id=\"areas-where-diffusion-models-have-been-applied-to\">Areas Where Diffusion Models have been applied to</h3><ol>  <li>Image Super Resolution</li>  <li>Image Inpainting</li>  <li>Image Outpainting</li>  <li>Image to Image generation</li>  <li>Semantic Segmentation</li>  <li>Point Cloud Completion and Generation</li>  <li>Text-to-Image Generation</li>  <li>Text-to-Audio Generation</li></ol><h2 id=\"state-of-the-art-diffusion-models-cherrypicked\">STATE OF THE ART DIFFUSION MODELS (cherrypicked)</h2><ol>  <li>DALLE 2</li>  <li>STABLE DIFFUSION</li></ol><h2 id=\"understanding-the-foundations-ddpms\">UNDERSTANDING THE FOUNDATIONS (DDPMs)</h2><p>Originally introduced by Sohl-Dickstein et al. (2015) in his paper “Deep unsupervised learning using nonequilibrium thermodynamics” which was followed up by Ho et al. (2020) in his paper “Denoising diffusion probabilistic models” (DDPMs), as well every other paper that has built on it. It is a latent variable generative model inspired by non-equilibrum thermodynamics in physics where samples generated is made possible by denoising process.</p><ol>  <li>Denoising Diffusion models is a two step model consisting of a forward process also called noising process and a backward process also know called denoisng or reverse noising process.</li>  <li>It works by adding gaussian noise to input data which could be an image following a Markov Chain.</li>  <li>The forward process follows a Markov chain where a little bit of gaussian noise is added to the input image which progressively disturbs the data distribution untill it is completely destroyed or unrecognised from gaussian noise.</li>  <li>The backward or reverse process learns to restore the data to its original form.</li>  <li>Reversing or removing the noise means recovering the values of the pixels, so that the resulting image will be similar to the original image. The reverse diffusion process takes a noisy image and learns to generate a less noisy version of that image, this process will be repeated until noise is converted to data. <strong>OR</strong></li>  <li>In order to generate new data, standard gaussian noise is used to perform the  denoising. This process will be repeated until noise is converted to data.</li>  <li>The recovering or noise reversal is parameterized because it uses a neural network. The task of the Neural Network is to predict the noise that was added in a given image.</li>  <li>The objective function of this model was simplified to a MSE Loss i.e given a noised image, the noise added is predicted and then this predicted noise is subtracted from the noise to get the real image. This is basically what is happenning when training a Diffusion Model, It is learning to denoise.</li>  <li>In DDPMs the forward process is fixed while the reverse process is what needs learning meaning we need to train only a single neural network.</li>  <li>The important components of a denoising diffusion mdoels include:    <ol>      <li>UNET Architecture</li>      <li>positional emebedidng</li>      <li>noise scheduler</li>      <li>attention mechanism</li>    </ol>  </li></ol><h2 id=\"mathematical-explanation\">MATHEMATICAL EXPLANATION</h2><h3 id=\"a-noising\">A. NOISING:</h3><h4 id=\"defining-the-process-formally\">DEFINING THE PROCESS FORMALLY</h4><h6 id=\"forward-process\">FORWARD PROCESS:</h6><p>The forward diffusion process starts from data (image) and generates this intermediate noisy images by simply adding noise one step at a timeAt every step, a normal distribution will be used to generate an image conditioned on the previous image.</p><table>  <tbody>    <tr>      <td>the normal distribution which is represented as $q(x_t</td>      <td>x_{t-1}) = \\mathcal{N(x_t; \\sqrt{1-\\beta_t{x_{t-1}}\\beta_t}I)}$ is going to take $x_{t-1}$ the prevoius step and generate $x_t$ the current step. It takes $x_0$ and it generates $x_1$</td>    </tr>  </tbody></table><p>A normal distribution over the current step $x_t$ where the mean is $(\\mathcal{\\sqrt{1-\\beta_t}})$ times the image at the prevoius time step which is ${x_{t-1}}$ and ${\\beta_t}I$ represents the variance scheduler which in the real sense is a very small positive scalar value $0.001$</p><p>This normal distribution, $\\mathcal{N(x_t; {\\sqrt{1-\\beta_t}{x_{t-1}}, \\beta_t}I)}$  takes the image at the previous step, rescales the pixel values in this image and then adds tiny bit of noise via the variance scheduler “per time step”</p><h6 id=\"joint-distribution\">JOINT DISTRIBUTION:</h6><p>A joint distibution can also be defined for all the samples generated in the forward process starting from $x_1$ all the way to $x_T$.  The joint distribution which is the samples conditioned on $x_0$ is the cumulateive product of the conditionals that are formed at each step as such $q(x_1,…,x_T|x_0)$ defines the joint distribution of all the samples that will be generated in the forward markov process\\(q(x_1,...,x_T|x_0) = \\prod^T_{t=1}   {q(x_t|x_{t-1})}\\)</p><h6 id=\"speed\">Speed?</h6><p>Why can’t we use $x_0$ our input image to generate noisy samples at any time step say $x_{10}$. Simply put can’t we use $x_0$ to generate $x_{10}$ ?. We can do that by making${\\alpha}_t$ = $1-\\beta_t$ , then $\\bar{\\alpha}_t$ which is the cumulative product of ${\\alpha}_t$ now becomes\\(\\bar{\\alpha}_t = \\prod^{t}_{s=1}(1-\\beta_s)\\)In order to answer the speed question we can then rewrite the original formular as follows:\t\t$q(x_t|x_0) = \\mathcal{N(x_t;\\sqrt{\\bar{\\alpha_t}}x_0,(1-{\\bar{\\alpha_t}})I)}$Using the reparameterization trick we can sample $x_t$ as follows$x_t$ = $\\sqrt{\\bar{\\alpha}}\\space x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\space \\epsilon$  where $\\epsilon \\sim{\\mathcal{N(0,1)}}$  and ${1-\\bar{\\alpha_t}}$ is our noise schedule at any time step, as such given $x_0$ we can draw samples at any time step $t$.</p><p>It should also be noted that the forward diffusion process is defined such that as  $(x_T\\mid{x_0})$ approaches infinity it becomes indistinguishable from standard normal distribution $\\mathcal{N({x_T;(0,1)})}$. ___</p><ol>  <li>The forward chain pertubs the data distribution by gradually adding Gaussian noise to the ground truth image with a pre-designed schedule until the data distribution converges to a given prior, i.e., a standard Gaussian Distribution – (Isometric Gaussian).\\(q(x_1,...,x_T|x_0) = \\prod^T_{t=1}   {q(x_t|x_{t-1})} ---(1)\\)\\(q(x_t|x_{t-1}) = \\mathcal{N(x_t; \\sqrt{1-\\beta_t{x_{t-1}}\\beta_t}I)}   ---(2)\\) $q(x_t)$ is used to denote the distributions of latent variables $x_t$ in the forward process.</li></ol><p>The noising process defined in Eq.(2) allows us to sample an arbitrary step of the noised latents directly conditioned on the input $x_o$.\tWhere $\\alpha_t$ = $1-\\beta_t$ and $\\bar{\\alpha_t}$ = $\\prod^t_{s=0}$ $\\bar{\\alpha_s}$, we can wite the marginal as:\t\\(q(x_t|x_0) = \\mathcal{N(x_t;\\sqrt{\\bar{\\alpha_t}}x_0,(1-{\\bar{\\alpha_t}})I)}\\)\\(x_t = \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-{\\bar{\\alpha_t}}}\\epsilon\\)When $\\bar{\\alpha}_t$ approximates 0, $x_T$ is practically indistinguisahble from pure Gaussian noise: $p(x_T)\\approx$ $\\mathcal{N}(x_T;0,1)$.</p><h3 id=\"b-denoising--what-is-means-to-reverse-the-noise\">B. DENOISING:  What is Means to Reverse The Noise?</h3><h4 id=\"denoising-defining-the-generative-model-by-denoising\">DENOISING: DEFINING THE GENERATIVE MODEL BY DENOISING</h4><p>In order to generate data from a diffusion model, we will start from pure noise which is a standard normal distribution with zero mean and unit variance and generates data by denoising one step at a time.</p><p>As such $p(x_T)$ = $\\mathcal{N(x_T;(0,1))}$  is the distribution of data at the end of the forward diffusion process.the parametric denoising distribution can be defined as follows $p_\\theta(x_{t-1}\\mid{x_t})$ = $\\mathcal{N({x_{t-1};{\\mu}<em>\\theta{(x_t,t)},{\\Sigma</em>\\theta({x_t,{t}}})})}$ apart from the sample $x_t$ at time $t$ the model also takes $t$ as input in order to account for the different noise levels at different time steps in the forward process noise schedule so that the model can learn to undo this individually</p><h4 id=\"joint-distribution-1\">Joint Distribution</h4><p>The joint distribution can be written as It is the product of the base distibution $p{(x_T)}$ and the product of the conditionals which still follows a markov process</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>\t$x_0\\Leftarrow \\cdots  \\Leftarrow \\cdots  \\Leftarrow x_{T-1}\\cdots  \\Leftarrow x_T$</code></pre></div></div>\\[p_\\theta(x_{0:T})=p(x_T)\\prod^T_{t=1}p_\\theta(x_{t-1}|x_t)\\]<p>The model is now tasked with learning the probability density of an earlier time step given the current timestep. During this process iteration is done from pure noise $x_T$ to $x_0$ our final image</p><p>Starting from sampled noise, the diffusion model performs $T$ denoising steps until a sharp image is formed.</p><p>The denosing process produces a series of intermediate images with decreasing levels of noise, denote as $x_T, x_{T-1},…x_0$,</p><p>Given only $x_T$ which is indistinguishable from gaussian noise we can get $x_0$ an output image.</p><p>The reverse process takes the completely noised image and learns to gradually revert the Markov chain of noise corruption to the ground truth. The reversed process is then written as follows: \\(p_\\theta({x_{t-1}|x_t}) = \\mathcal{N}(x_{t-1};\\mu(x_t,t, {\\Sigma}_\\theta(x_t,t))\\)<em>__In Denoising Diffusion Models,  The Noise $\\epsilon$ is what is Predicted and this is done by Optimizing the variational upper bound on the negative log-likelihood.__</em>\\(\\\\E\\ [-\\log{p_{\\theta}}(x_0)]&lt; \\\\ E_q[-\\log\\frac{p_\\theta({x_{0:T}})}{q(x_{1:T}|x_0)}]--(7)\\)\\(\\\\ E_q[-\\log{p{(x_T)}}-\\sum_{t&gt;=1}{\\log\\frac{p_\\theta({x_{0:T}})}{q(x_{1:T}|x_0)}} ]-- (8)\\)\\(= -L_{VLB}\\)<em>__Reparametrization have been appplied to Eq. (8), which results in the general objective below:\\(E_{t\\sim{\\mathcal{U(0,T),x_0{\\sim{q{(x_0),\\epsilon\\sim{\\mathcal{N(0,1)}}}}}}}}[\\lambda{(t)}||\\epsilon-{\\epsilon_\\theta}(x_t,t)||^2]   --(10)\\)__</em>The neural network ${\\epsilon_\\theta}(x_t,t)$ predicts $\\epsilon$ by minimizing the loss = ${||\\epsilon-\\epsilon_\\theta}(x_t,t)||^2$ which is the $L_2$ Loss \tINTUITIVELY: Given a noised image, the noise added is predicted and then this predicted noise is subtracted from the noise to get the real image. This is basically what is happenning when training a Diffusion Model, It is learning to denoise.</p><hr /><hr /><p>Both the forward Diffusion processes $q(x_t|x_{t-1})$ and the backward or reconstruction process $q(x_{t-1}|x_t)$ are modelled as the products of Markov transition probabilities:\\(q(x_{0:T}) = q(x_0)\\prod_{t=1}^T{q(x_t|x_{t-1})}, p_\\theta(x_{T:0}) = p(x_{T})\\prod_{t=T}^1{p_{\\theta}(x_{t-1}|x_t)},\\)$q(x_0)$ is the real data distribution</p><h4 id=\"important\">IMPORTant</h4><p>Diffusion models are latent variable models\tLatent variables$:$ = $x_1,x_{2},x_3,x_4,\\cdots x_T$<br />\tObserved variables$:$ $x_0$</p><h4 id=\"training-a-denosing-diffusion-probabilistic-model\">TRAINING A DENOSING DIFFUSION PROBABILISTIC MODEL</h4><p>The reverse step process is only tasked with learning the mean while its variance is set to a constant<strong>__</strong></p><h5 id=\"objective-function-of-a-ddpm\">OBJECTIVE FUNCTION OF A DDPM</h5><p>\\(E_{x_0{\\sim{q{(x_0),\\epsilon\\sim{\\mathcal{N(0,1)t\\sim{\\mathcal{U(0,T),}}}}}}}}[||\\epsilon-{\\epsilon_\\theta}(\\sqrt{\\bar{\\alpha}}\\space x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\space \\epsilon),t||^2]\\)\tWhere $x_t = \\sqrt{\\bar{\\alpha}}\\space x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\space \\epsilon)$</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>                     $E_{x_0{\\sim{q{(x_0),\\epsilon\\sim{\\mathcal{N(0,1)t\\sim{\\mathcal{U(0,T),}}}}}}}}[||\\epsilon-{\\epsilon_\\theta}(x_t,t)||^2]$</code></pre></div></div><p>Our loss funtion finally looks like this$:$\t\t\t                $L_{simple}={E_{x_0,t,\\epsilon}}[||\\epsilon_-\\epsilon_{\\theta}(x_t,t)||^2]$<strong>__</strong>__</p><h6 id=\"the-training-algorithm-looks-like-this-traindiffpng\">The Training Algorithm looks like this: ![[trainDIFF.PNG]]</h6><h4 id=\"image-generation-or-samling-from-a-denosing-diffusion-probabilistic-model\">IMAGE GENERATION or SAMLING FROM A DENOSING DIFFUSION PROBABILISTIC MODEL</h4><hr /><h6 id=\"the-sampling-algorithm-looks-like-this-samplediffpng\">The Sampling Algorithm looks like this: ![[sampleDIFF.PNG]]</h6><h2 id=\"important-components-of-a-denoising-diffusion-model\">IMPORTANT COMPONENTS OF A DENOISING DIFFUSION MODEL</h2><ol>  <li>    <p>UNET ARCHITECTURE: The UNET Architecture is a Convolutional Network originally developed for biomedical image segmentation. It has a U Shape and it does downsampling on one part and upsampling on the other side and all its operation are convolution based.</p>  </li>  <li>    <p>POSITIONAL EMBEDDING: Positional embedding as originally used in the Attention is all you need paper was utilized for train the neural network has shared parameters across time which means it can’t distinguish between various timesteps as such it needs to filter the noise across images with different noise intensities.</p>    <p>PE is added as a way of encoding positional information into the UNET Model to help distinguish the various noise intensities within the markov chain process</p>    <p>The positional embedding is added as additional information in the downsample, middle and upsample block of the UNET Model.</p>    <p>PE are wave frequencies used to capture positonal information at both odd $sin()$ and even $cosine()$ positions and this embeddings can be calculated as follows.</p>    <p>$PE(<em>{pos,2i}) = sin(pos\\div{1000^{2i/d}})$ $PE(</em>{pos,2i+1}) = cos(pos\\div{1000^{2i/d}})$</p>  </li>  <li>    <p>NOISE SCHEDULER: This is just a technique that makes possible an iterative addition of noise to an image of adding noise.</p>  </li>  <li>    <p>ATTENTION MECHANISM: It is a mechanism that makes a model selectively focus on the varying parts of the inputs. It assigns weights to the different positions which indicates the importance of the input sequence for generating outputs by calculating attention scores. Originally in the transformers it was used both in the encoder and decoder parts of the Model.</p>  </li></ol><h2 id=\"implementing-denoisng-diffusion-model-with-pytorch\">IMPLEMENTING DENOISNG DIFFUSION MODEL WITH PYTORCH</h2><p>It should be noted that I will not run training because it is needless and basically useless to train this kind of model because it is not state of the art as compared to the other models briefly explained above except you have a gpu compute where you can run it for over 1000 epochs. In <strong>part 2</strong> of this article you will see a detailed handson code for finetuning stable diffusion using dreambooth.</p><pre><code class=\"language-jupyter\">import torchfrom torch import nnfrom torch.nn import functional as Fimport numpy as npimport matplotlib.pyplot as pltimport torchvisionfrom torchvision import transformsfrom torch.utils.data import DataLoaderfrom tqdm.notebook import tqdmfrom torch.utils.data import Subsetimport copy</code></pre><pre><code class=\"language-jupyter\">def show_images(images, title=\"\"):    \"\"\"Shows the provided images as sub-pictures in a square\"\"\"    images = [np.clip(im.permute(1,2,0).numpy(),0,1) for im in images]      # Defining number of rows and columns    fig = plt.figure(figsize=(8, 8))    rows = int(len(images) ** (1 / 2))    cols = round(len(images) / rows)     # Populating figure with sub-plots    idx = 0    for r in range(rows):        for c in range(cols):            fig.add_subplot(rows, cols, idx + 1)            if idx &lt; len(images):                plt.imshow(images[idx])                plt.axis('off')                idx += 1    fig.suptitle(title, fontsize=30)    # Showing the figure    plt.show()</code></pre><pre><code class=\"language-jupyter\">device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")device</code></pre><pre><code class=\"language-jupyter\">def sinusoidal_embedding(n, d):    # Returns the standard positional embedding    embedding = torch.tensor([[i / 10_000 ** (2 * j / d) for j in range(d)] for i in range(n)])    sin_mask = torch.arange(0, n, 2)    embedding[sin_mask] = torch.sin(embedding[sin_mask])    embedding[1 - sin_mask] = torch.cos(embedding[sin_mask])    return embedding</code></pre><pre><code class=\"language-jupyter\">class double_conv(nn.Module):    def __init__(self, in_ch, out_ch):        super(double_conv, self).__init__()        self.conv = nn.Sequential(            nn.Conv2d(in_ch, out_ch, 3, padding=1),            nn.BatchNorm2d(out_ch),            nn.ReLU(inplace=True),            nn.Conv2d(out_ch, out_ch, 3, padding=1),            nn.BatchNorm2d(out_ch),            nn.ReLU(inplace=True)        )      def forward(self, x):        x = self.conv(x)        return xclass down_layer(nn.Module):    def __init__(self, in_ch, out_ch):        super(down_layer, self).__init__()        self.pool = nn.MaxPool2d(2, stride=2, padding=0)        self.conv = double_conv(in_ch, out_ch)    def forward(self, x):        x = self.conv(self.pool(x))        return x</code></pre><pre><code class=\"language-jupyter\">class up(nn.Module):    def __init__(self, in_ch, out_ch):        super(up, self).__init__()        self.up_scale = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)      def forward(self, x1, x2): # x1 (bs,out_ch,w1,h1) x2 (bs,in_ch,w2,h2)        x2 = self.up_scale(x2) # (bs,out_ch,2*w2,2*h2)        diffY = x1.size()[2] - x2.size()[2]        diffX = x1.size()[3] - x2.size()[3]        x2 = F.pad(x2, [diffX // 2, diffX - diffX // 2,                        diffY // 2, diffY - diffY // 2]) # (bs,out_ch,w1,h1)        x = torch.cat([x2, x1], dim=1) # (bs,2*out_ch,w1,h1)        return xclass up_layer(nn.Module):    def __init__(self, in_ch, out_ch): # !! 2*out_ch = in_ch !!        super(up_layer, self).__init__()        self.up = up(in_ch, out_ch)        self.conv = double_conv(in_ch, out_ch)      def forward(self, x1, x2): # x1 (bs,out_ch,w1,h1) x2 (bs,in_ch,w2,h2)        a = self.up(x1, x2) # (bs,2*out_ch,w1,h1)        x = self.conv(a) # (bs,out_ch,w1,h1) because 2*out_ch = in_ch        return x</code></pre><pre><code class=\"language-jupyter\">class UNet(nn.Module):    def __init__(self, in_channels=1, n_steps=1000, time_emb_dim=100):        super(UNet, self).__init__()        self.conv1 = double_conv(in_channels, 64)        self.down1 = down_layer(64, 128)        self.down2 = down_layer(128, 256)        self.down3 = down_layer(256, 512)        self.down4 = down_layer(512, 1024)        self.up1 = up_layer(1024, 512)        self.up2 = up_layer(512, 256)        self.up3 = up_layer(256, 128)        self.up4 = up_layer(128, 64)        self.last_conv = nn.Conv2d(64, in_channels, 1)        # Time embedding        self.time_embed = nn.Embedding(n_steps, time_emb_dim)        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)        self.time_embed.requires_grad_(False)        self.te1 = self._make_te(time_emb_dim, in_channels)        self.te2 = self._make_te(time_emb_dim, 64)        self.te3 = self._make_te(time_emb_dim, 128)        self.te4 = self._make_te(time_emb_dim, 256)        self.te5 = self._make_te(time_emb_dim, 512)        self.te1_up = self._make_te(time_emb_dim, 1024)        self.te2_up = self._make_te(time_emb_dim, 512)        self.te3_up = self._make_te(time_emb_dim, 256)        self.te4_up = self._make_te(time_emb_dim, 128)      def _make_te(self, dim_in, dim_out):        return nn.Sequential(nn.Linear(dim_in, dim_out), nn.SiLU(), nn.Linear(dim_out, dim_out))    def forward(self, x , t): # x (bs,in_channels,w,d)        bs = x.shape[0]        t = self.time_embed(t)        x1 = self.conv1(x+self.te1(t).reshape(bs, -1, 1, 1)) # (bs,64,w,d)        x2 = self.down1(x1+self.te2(t).reshape(bs, -1, 1, 1)) # (bs,128,w/2,d/2)        x3 = self.down2(x2+self.te3(t).reshape(bs, -1, 1, 1)) # (bs,256,w/4,d/4)        x4 = self.down3(x3+self.te4(t).reshape(bs, -1, 1, 1)) # (bs,512,w/8,h/8)        x5 = self.down4(x4+self.te5(t).reshape(bs, -1, 1, 1)) # (bs,1024,w/16,h/16)        x1_up = self.up1(x4, x5+self.te1_up(t).reshape(bs, -1, 1, 1)) # (bs,512,w/8,h/8)        x2_up = self.up2(x3, x1_up+self.te2_up(t).reshape(bs, -1, 1, 1)) # (bs,256,w/4,h/4)        x3_up = self.up3(x2, x2_up+self.te3_up(t).reshape(bs, -1, 1, 1)) # (bs,128,w/2,h/2)        x4_up = self.up4(x1, x3_up+self.te4_up(t).reshape(bs, -1, 1, 1)) # (bs,64,w,h)        output = self.last_conv(x4_up) # (bs,in_channels,w,h)        return output</code></pre><pre><code class=\"language-jupyter\">bs = 3x = torch.randn(bs,1,32,32)n_steps=1000timesteps = torch.randint(0, n_steps, (bs,)).long()unet = UNet()</code></pre><pre><code class=\"language-jupyter\">y = unet(x,timesteps)y.shape</code></pre><pre><code class=\"language-jupyter\">class DDPM(nn.Module):    def __init__(self, network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device) -&gt; None:        super(DDPM, self).__init__()        self.num_timesteps = num_timesteps        self.betas = torch.linspace(beta_start, beta_end, num_timesteps, dtype=torch.float32).to(device)        self.alphas = 1.0 - self.betas        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)        self.network = network        self.device = device        self.sqrt_alphas_cumprod = self.alphas_cumprod ** 0.5 # used in add_noise        self.sqrt_one_minus_alphas_cumprod = (1 - self.alphas_cumprod) ** 0.5 # used in add_noise and step      def add_noise(self, x_start, x_noise, timesteps):        # The forward process        # x_start and x_noise (bs, n_c, w, d)        # timesteps (bs)        s1 = self.sqrt_alphas_cumprod[timesteps] # bs        s2 = self.sqrt_one_minus_alphas_cumprod[timesteps] # bs        s1 = s1.reshape(-1,1,1,1) # (bs, 1, 1, 1) for broadcasting        s2 = s2.reshape(-1,1,1,1) # (bs, 1, 1, 1)        return s1 * x_start + s2 * x_noise    def reverse(self, x, t):        # The network return the estimation of the noise we added        return self.network(x, t)            def step(self, model_output, timestep, sample):        # one step of sampling        # timestep (1)        t = timestep        coef_epsilon = (1-self.alphas)/self.sqrt_one_minus_alphas_cumprod        coef_eps_t = coef_epsilon[t].reshape(-1,1,1,1)        coef_first = 1/self.alphas ** 0.5        coef_first_t = coef_first[t].reshape(-1,1,1,1)        pred_prev_sample = coef_first_t*(sample-coef_eps_t*model_output)        variance = 0        if t &gt; 0:            noise = torch.randn_like(model_output).to(self.device)            variance = ((self.betas[t] ** 0.5) * noise)        pred_prev_sample = pred_prev_sample + variance        return pred_prev_sample</code></pre><pre><code class=\"language-jupyter\">def training_loop(model, dataloader, optimizer, num_epochs, num_timesteps, device=device):    \"\"\"Training loop for DDPM\"\"\"    global_step = 0    losses = []    for epoch in range(num_epochs):        model.train()        progress_bar = tqdm(total=len(dataloader))        progress_bar.set_description(f\"Epoch {epoch}\")        for step, batch in enumerate(dataloader):            batch = batch[0].to(device)            noise = torch.randn(batch.shape).to(device)            timesteps = torch.randint(0, num_timesteps, (batch.shape[0],)).long().to(device)              noisy = model.add_noise(batch, noise, timesteps)            noise_pred = model.reverse(noisy, timesteps)            loss = F.mse_loss(noise_pred, noise)            optimizer.zero_grad()            loss.backward()            optimizer.step()            progress_bar.update(1)            logs = {\"loss\": loss.detach().item(), \"step\": global_step}            losses.append(loss.detach().item())            progress_bar.set_postfix(**logs)            global_step += 1        progress_bar.close()</code></pre><pre><code class=\"language-jupyter\">root_dir = './'transforms01 = torchvision.transforms.Compose([        torchvision.transforms.ToTensor(),        #torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))    ])dataset = torchvision.datasets.CIFAR10(root=root_dir, train=True, transform=transforms01, download=True)dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=512, shuffle=True,num_workers=10)</code></pre><pre><code class=\"language-jupyter\">for b in dataloader:    batch = b[0]    break    bn = [b for b in batch[:100]]show_images(bn, \"origin\")</code></pre><p>![[res.png]]</p><pre><code class=\"language-jupyter\">learning_rate = 1e-3num_epochs = 15num_timesteps = 1000network = UNet(in_channels=3)network.to(device)model = DDPM(network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device)model.train()optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)training_loop(model, dataloader, optimizer, num_epochs, num_timesteps, device=device)</code></pre><p>Model will start training for about 15 epochs</p><pre><code class=\"language-jupyter\">def generate_image(ddpm, sample_size, channel, size):    \"\"\"Generate the image from the Gaussian noise\"\"\"      frames = []    frames_mid = []    ddpm.eval()    with torch.no_grad():        timesteps = list(range(ddpm.num_timesteps))[::-1]        sample = torch.randn(sample_size, channel, size, size).to(device)        for i, t in enumerate(tqdm(timesteps)):            time_tensor = (torch.ones(sample_size) * t).long().to(device)            residual = ddpm.reverse(sample, time_tensor).to(device)            sample = ddpm.step(residual, time_tensor[0], sample)            if t==500:                #sample_squeezed = torch.squeeze(sample)                for i in range(sample_size):                    frames_mid.append(sample[i].detach().cpu())        #sample = torch.squeeze(sample)        for i in range(sample_size):            frames.append(sample[i].detach().cpu())    return frames, frames_mid</code></pre><pre><code class=\"language-jupyter\">def make_dataloader(dataset, class_name ='ship'):    s_indices = []    s_idx = dataset.class_to_idx[class_name]    for i in range(len(dataset)):        current_class = dataset[i][1]        if current_class == s_idx:            s_indices.append(i)    s_dataset = Subset(dataset, s_indices)    return torch.utils.data.DataLoader(dataset=s_dataset, batch_size=512, shuffle=True)</code></pre><pre><code class=\"language-jupyter\">ship_dataloader = make_dataloader(dataset)</code></pre><pre><code class=\"language-jupyter\">ship_network = copy.deepcopy(network)ship_model = DDPM(ship_network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device)num_epochs = 10num_timesteps = model.num_timestepslearning_rate = 1e-3ship_model.train()optimizer = torch.optim.Adam(ship_model.parameters(), lr=learning_rate)training_loop(ship_model, ship_dataloader, optimizer, num_epochs, num_timesteps, device=device)</code></pre><pre><code class=\"language-jupyter\">generated, generated_mid = generate_image(ship_model, 100, 3, 32)</code></pre><pre><code class=\"language-jupyter\">show_images(generated, \"Generated ships\")</code></pre><p>![[ddpm.png]]</p><p>This training was only done for very few epochs thats why we still do not have a very detailed generated result.</p><h2 id=\"conclusions\">CONCLUSIONS</h2><p>This article covered the foundations of Diffusion model by going deeply into Denoising diffusion models, the concepts, the maths and the code. There’s actually no point training this kind of an AI model because it wont result in any generation that is as good as DALL-E or Stable Diffusion in terms of Fidelity and Diveristy. As such we have SOTA Models that can perform text to image generation better than this one even though the purpose of this article was just to give an overview of the building blocks of a diffusion model.<em>__</em></p><h2 id=\"references\">REFERENCES</h2><ol>  <li>Karsten Kreis; Ruigui Gao; Arash Vahdat (2022-5-4): “Denoising Diffusion-based Generative Modelling: Foundations and Applications (CVPR 2002 Worskshop) “</li>  <li>Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models, 2020.</li>  <li>Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics, 2015.</li>  <li>https://github.com/dataflowr/notebooks/blob/master/Module18/ddpm_micro_sol.ipynb</li>  <li>Karsten Kreis; Ruigui Gao; Arash Vahdat (2022-5-4): “Denoising Diffusion-based Generative Modelling: Foundations and Applications (CVPR 2002 Worskshop) “</li></ol>",
            "url": "https://aidyai.github.io//Gesko/2022/12/20/understanding-diffusion-models",
            
            
            
            
            
            "date_published": "2022-12-20T00:00:00+01:00",
            "date_modified": "2022-12-20T00:00:00+01:00",
            
                "author":  {
                "name": "gesko",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "https://aidyai.github.io//Gesko/2019/08/31/difference-between-font-formats",
            "title": "Difference between font formats",
            "summary": "What is the difference between various font formats?",
            "content_text": "h1h2h3h4h5Lorem ipsum dolor amet tousled viral art party blue bottle single-origin coffee cardigan, selvage man braid helvetica. Banh mi taxidermy meditation microdosing. Selvage cornhole YOLO, small batch vexillologist raclette VHS prism sustainable 8-bit ugh semiotics letterpress disrupt pop-up. Celiac shabby chic ugh, jianbing whatever kitsch tattooed edison bulb kogi irony etsy.  a  b  c  Franzen polaroid hammock iceland blue bottle woke disrupt tilde kale chips raw denim ramps vaporware before they sold out irony. Narwhal vaporware offal shaman celiac kinfolk activated charcoal salvia lomo irony readymade normcore. Yr activated charcoal kombucha, man braid whatever biodiesel hella crucifix adaptogen bicycle rights small batch skateboard mixtape. Hot chicken sustainable green juice 90’s. Ennui kickstarter hella pug, meggings man bun shaman messenger bag. Chambray adaptogen kombucha pug affogato, kogi green juice distillery ugh banh mi.VHS roof party waistcoat cold-pressed, snippet.CodeExample(); master cleanse affogato franzen. Shaman iceland pour-over intelligentsia typewriter tilde, pitchfork copper mug. Wayfarers kickstarter adaptogen vinyl beard kombucha. Organic pinterest master cleanse, mixtape fam gentrify lo-fi kogi.fun main(args: Array&lt;String&gt;) {    println(\"Hello, World!\")}Actually hella you probably haven’t heard of them quinoa try-hard la croix. Street art schlitz actually hell of pour-over air plant. Post-ironic franzen brunch mumblecore readymade. Food truck photo booth polaroid, gochujang vegan street art yr before they sold out man bun. Tilde selfies chia pitchfork everyday carry post-ironic mumblecore sartorial VHS master cleanse activated charcoal biodiesel williamsburg cronut jean shorts. Poutine helvetica keffiyeh butcher pop-up.",
            "content_html": "<h1 id=\"h1\">h1</h1><h2 id=\"h2\">h2</h2><h3 id=\"h3\">h3</h3><h4 id=\"h4\">h4</h4><h5 id=\"h5\">h5</h5><hr /><p>Lorem ipsum dolor amet tousled viral art party blue bottle single-origin coffee cardigan, selvage man braid helvetica. <a href=\"//#\">Banh</a> mi taxidermy meditation microdosing. Selvage cornhole YOLO, small batch vexillologist raclette VHS prism sustainable 8-bit ugh semiotics letterpress disrupt pop-up. Celiac shabby chic ugh, jianbing whatever kitsch tattooed edison bulb kogi irony etsy.</p><ul>  <li>a</li>  <li>b</li>  <li>c</li></ul><blockquote>  <p>Franzen polaroid hammock iceland blue bottle woke disrupt tilde kale chips raw denim ramps vaporware before they sold out irony. Narwhal vaporware offal shaman celiac kinfolk activated charcoal salvia lomo irony readymade normcore. Yr activated charcoal kombucha, man braid whatever biodiesel hella crucifix adaptogen bicycle rights small batch skateboard mixtape. Hot chicken sustainable green juice 90’s. Ennui kickstarter hella pug, meggings man bun shaman messenger bag. Chambray adaptogen kombucha pug affogato, kogi green juice distillery ugh banh mi.</p></blockquote><p>VHS roof party waistcoat cold-pressed, <code class=\"language-plaintext highlighter-rouge\">snippet.CodeExample();</code> master cleanse affogato franzen. Shaman iceland pour-over intelligentsia typewriter tilde, pitchfork copper mug. Wayfarers kickstarter adaptogen vinyl beard kombucha. Organic pinterest master cleanse, mixtape fam gentrify lo-fi kogi.</p><div class=\"language-java highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">fun</span> <span class=\"nf\">main</span><span class=\"o\">(</span><span class=\"nl\">args:</span> <span class=\"nc\">Array</span><span class=\"o\">&lt;</span><span class=\"nc\">String</span><span class=\"o\">&gt;)</span> <span class=\"o\">{</span>    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">\"Hello, World!\"</span><span class=\"o\">)</span><span class=\"o\">}</span></code></pre></div></div><p>Actually hella you probably haven’t heard of them quinoa try-hard la croix. Street art schlitz actually hell of pour-over air plant. Post-ironic franzen brunch mumblecore readymade. Food truck photo booth polaroid, gochujang vegan street art yr before they sold out man bun. Tilde selfies chia pitchfork everyday carry post-ironic mumblecore sartorial VHS master cleanse activated charcoal biodiesel williamsburg cronut jean shorts. Poutine helvetica keffiyeh butcher pop-up.</p>",
            "url": "https://aidyai.github.io//Gesko/2019/08/31/difference-between-font-formats",
            
            
            
            
            
            "date_published": "2019-08-31T00:00:00+02:00",
            "date_modified": "2019-08-31T00:00:00+02:00",
            
                "author":  {
                "name": "gesko",
                "url": null,
                "avatar": null
                }
                
            
        }
    
    ]
}