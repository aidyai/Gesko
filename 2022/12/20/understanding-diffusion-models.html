<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="https://p0wex.github.io/Gesko/assets/css/syntax.css"><link rel="preconnect" href="https://cdnjs.cloudflare.com"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"><title> Understanding diffusion models - Gesko</title><link rel="shortcut icon" href="https://p0wex.github.io/Gesko/favicon.png"><link rel="alternate" type="application/atom+xml" title="Gesko" href="https://p0wex.github.io/Gesko/atom.xml"><link rel="alternate" type="application/json" title="Gesko" href="https://p0wex.github.io/Gesko/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="https://p0wex.github.io/Gesko/sitemap.xml" /><link rel="preconnect" href="https://fonts.gstatic.com"><link href="https://fonts.googleapis.com/css2?family=Play:wght@400;700&family=Source+Code+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,300;1,400;1,500;1,600;1,700;1,900&display=swap" rel="stylesheet"><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}html{font-size:15px}#theme-toggle{opacity:0.65;position:relative;border-radius:5px;height:25px;display:flex;-webkit-box-align:center;align-items:center;-webkit-box-pack:center;justify-content:center;transition:opacity 0.3s ease 0s;border:none;outline:none;background:none;cursor:pointer;padding:0px;appearance:none;transform:scale(0.8)}html,html[data-theme="light"]{--text-font: Play;--source-code-font: Source Code Pro;--very-light-text-color: rgb(222, 222, 222);--light-text-color: rgb(89, 183, 255);--medium-text-color: #555;--main-text-color: #333;--highlight-text-color: #6cb4a0;--dark-text-color: #393e46;--link-color: rgb(50, 87, 209);--code-bg-color: rgb(245, 245, 245);background-color:#fff}html #theme-toggle div,html[data-theme="light"] #theme-toggle div{position:relative;width:24px;height:24px;border-radius:50%;border:none;background-color:var(--theme-ui-colors-transparent, transparent);transform:scale(1);transition:all 0.45s ease 0s;overflow:hidden;box-shadow:inset 8px -8px 0px 0px var(--theme-ui-colors-toggleIcon, #2d3748)}html #theme-toggle div::before,html[data-theme="light"] #theme-toggle div::before{content:"";position:absolute;right:-9px;top:-9px;height:24px;width:24px;border:none;border-radius:50%;transform:translate(0px, 0px);opacity:1;transition:transform 0.45s ease 0s}html #theme-toggle div::after,html[data-theme="light"] #theme-toggle div::after{content:"";width:8px;height:8px;border-radius:50%;margin:-4px 0px 0px -4px;position:absolute;top:50%;left:50%;box-shadow:0 -23px 0 var(--theme-ui-colors-toggleIcon, #2d3748),0 23px 0 var(--theme-ui-colors-toggleIcon, #2d3748),23px 0 0 var(--theme-ui-colors-toggleIcon, #2d3748),-23px 0 0 var(--theme-ui-colors-toggleIcon, #2d3748),15px 15px 0 var(--theme-ui-colors-toggleIcon, #2d3748),-15px 15px 0 var(--theme-ui-colors-toggleIcon, #2d3748),15px -15px 0 var(--theme-ui-colors-toggleIcon, #2d3748),-15px -15px 0 var(--theme-ui-colors-toggleIcon, #2d3748);transform:scale(0);transition:all 0.35s ease 0s}html[data-theme="dark"]{--text-font: Play;--source-code-font: Source Code Pro;--very-light-text-color: #EEE;--light-text-color: #c4bbf0;--medium-text-color: #52057b;--main-text-color: #EAEAEA;--highlight-text-color: #6cb4a0;--dark-text-color: rgb(36, 38, 42);--link-color: rgb(255, 89, 125);--code-bg-color: rgb(36, 38, 42);background-color:#131418}html[data-theme="dark"] #theme-toggle div{position:relative;width:24px;height:24px;border-radius:50%;border:4px solid var(--theme-ui-colors-toggleIcon, #cbd5e0);background-color:var(--theme-ui-colors-toggleIcon, #cbd5e0);transform:scale(0.55);transition:all 0.45s ease 0s;overflow:visible;box-shadow:none}html[data-theme="dark"] #theme-toggle div::before{content:"";position:absolute;right:-9px;top:-9px;height:24px;width:24px;border:2px solid var(--theme-ui-colors-toggleIcon, #cbd5e0);border-radius:50%;transform:translate(14px, -14px);opacity:0;transition:transform 0.45s ease 0s}html[data-theme="dark"] #theme-toggle div::after{content:"";width:8px;height:8px;border-radius:50%;margin:-4px 0px 0px -4px;position:absolute;top:50%;left:50%;box-shadow:0 -23px 0 var(--theme-ui-colors-toggleIcon, #cbd5e0),0 23px 0 var(--theme-ui-colors-toggleIcon, #cbd5e0),23px 0 0 var(--theme-ui-colors-toggleIcon, #cbd5e0),-23px 0 0 var(--theme-ui-colors-toggleIcon, #cbd5e0),15px 15px 0 var(--theme-ui-colors-toggleIcon, #cbd5e0),-15px 15px 0 var(--theme-ui-colors-toggleIcon, #cbd5e0),15px -15px 0 var(--theme-ui-colors-toggleIcon, #cbd5e0),-15px -15px 0 var(--theme-ui-colors-toggleIcon, #cbd5e0);transform:scale(1);transition:all 0.35s ease 0s}body{font-family:var(--text-font),monospace;text-rendering:optimizeLegibility;line-height:1.75;color:var(--main-text-color)}a{color:var(--link-color);text-decoration:none}.post p{margin:1rem 0}.meta{margin:1.4rem 0}code,pre{background:var(--code-bg-color);margin:0 -27px;border-width:1px}code{font-family:var(--source-code-font),monospace;margin:unset;padding:0.1rem;color:var(--highlight-text-color)}pre code{background-color:unset;color:unset}pre{border-width:3px;padding:0.8rem;white-space:pre-wrap;border-style:none none none solid;border-color:var(--light-text-color)}img{max-width:100%}hr{background:var(--light-text-color);height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:0.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem 0}section{margin-top:5.5rem}blockquote{font-style:italic;border-left:5px solid var(--dark-text-color);padding-left:1rem}h1{color:var(--main-text-color);text-transform:uppercase;font-size:2rem;font-weight:bold;margin-bottom:1.5rem}h2,h3,h4,h5{font-weight:bold;margin-bottom:1.5rem;font-size:2rem}h2{font-size:1.8rem}h3{font-size:1.6rem}h4{font-size:1.4rem}h5{font-size:1.2rem}h6{font-size:1rem}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts>h3{font-weight:500}.post ul{margin:0px;margin:0px 0px 10px 10px}.post ol{margin:0px;margin:0px 0px 10px 15px}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:0.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;font-weight:bold}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}time{color:var(--main-text-color)}.post>h1.title{font-size:x-large;margin-top:0;margin-bottom:0}.post>time{margin-bottom:0;margin-top:3rem}.highlight{margin:0px}main{display:flex;max-width:47rem;margin:-5rem auto 0;padding:2.4rem 2rem;flex-direction:column}.flex-row-between{display:flex;flex-direction:row;justify-content:space-between}.social-footer{padding:1rem;display:flex;justify-content:center;position:initial}.social-footer .social-footer-icons .fa{font-size:1.3rem;color:#a9a9a9}.social-footer .social-footer-icons .fa:hover{color:dimgray;transition:color 0.3s ease-in}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}main{padding:0 2rem}}section{flex-grow:999;display:flex;flex-direction:column}figcaption{font-size:smaller}.bio{margin-bottom:5rem}::selection{background-color:#fffba0;color:#333}.search-article{position:relative}.search-article label[for="search-input"]{position:relative;top:-10px;left:11px}.search-article input[type="search"]{top:-1rem;left:0;border:0;width:100%;height:30px;outline:none;position:absolute;border-radius:5px;padding:10px 10px 10px 35px;color:var(--main-text-color);-webkit-appearance:none;background-color:rgba(128,128,128,0.1);border:1px solid rgba(128,128,128,0.1)}.search-article input[type="search"]::-webkit-input-placeholder{color:#808080}.search-article input[type="search"]::-webkit-search-decoration,.search-article input[type="search"]::-webkit-search-results-decoration{display:none}#search-results{text-align:center}#search-results li{text-align:center}#search-results li::before{content:"›";display:inline-block;margin-left:-1.3em;width:1.3em;color:var(--main-text-color)}.post-nav{display:flex;position:relative;margin-top:0;border-top:2px solid var(--light-text-color);line-height:1.4}.post-nav .post-nav-item{border-bottom:0;padding-bottom:10px;width:50%;padding-top:10px;text-decoration:none;box-sizing:border-box}.post-nav .post-nav-item .post-title{color:var(--main-text-color)}.post-nav .post-nav-item:hover .post-title,.post-nav .post-nav-item:focus .post-title{color:var(--link-color);opacity:0.9}.post-nav .post-nav-item .nav-arrow{font-size:17px;color:var(--main-text-color);margin-bottom:3px;font-weight:bold}.post-nav .post-nav-item:nth-child(odd){padding-left:0;padding-right:20px}.post-nav .post-nav-item:nth-child(even){text-align:right;padding-right:0;padding-left:20px}.tags ul{list-style:none;display:grid;grid-template-columns:auto auto auto auto auto auto;grid-template-rows:auto auto auto auto}.tags li{white-space:normal;text-overflow:ellipsis}.timeline{position:center;width:650px;margin:-6em auto;padding:1px;list-style-type:none}@media (max-width: 860px){.timeline{width:100%}}.timeline:before{position:absolute;left:50%;top:0;content:' ';display:block;width:6px;height:100%;margin-left:-3px;background:var(--light-text-color);z-index:5}@media (max-width: 375px){.timeline:before{height:150%}}.timeline li{padding:2em 0}@media (max-width: 860px){.timeline li{padding:2em 0}}.timeline li::after{content:"";display:block;clear:both;visibility:hidden}.direction-l{position:relative;width:287px;float:left;text-align:right}@media (max-width: 860px){.direction-l{float:none;width:100%;text-align:center}}.direction-l .flag{color:#333;box-shadow:-1px 1px 1px rgba(0,0,0,0.52)}.direction-l .flag:after{content:"";position:absolute;left:100%;top:50%;height:0;width:0;margin-top:-8px;border:solid transparent;border-left-color:#f8f8f8;border-width:8px;pointer-events:none}@media (max-width: 860px){.direction-l .flag:after{content:"";position:absolute;left:50%;top:-8px;height:0;width:0;margin-left:-8px;border:solid transparent;border-bottom-color:#fff;border-width:8px;pointer-events:none}}.direction-l .time-wrapper{float:left}@media (max-width: 860px){.direction-l .time-wrapper{float:none}}.direction-r{position:relative;width:293px;float:right;text-align:left}@media (max-width: 860px){.direction-r{float:none;width:100%;text-align:center}}.direction-r .flag{color:#333;box-shadow:1px 1px 1px rgba(0,0,0,0.52)}.direction-r .flag:after{content:"";position:absolute;right:100%;top:50%;height:0;width:0;margin-top:-8px;border:solid transparent;border-right-color:#f8f8f8;border-width:8px;pointer-events:none}@media (max-width: 860px){.direction-r .flag:after{content:"";position:absolute;left:50%;top:-8px;height:0;width:0;margin-left:-8px;border:solid transparent;border-bottom-color:#fff;border-width:8px;pointer-events:none}}.direction-r .flag:before{left:-40px}.direction-r .time-wrapper{float:right}@media (max-width: 860px){.direction-r .time-wrapper{float:none}}.flag-wrapper{position:relative;display:inline-block;text-align:center}@media (max-width: 860px){.flag-wrapper{text-align:center}}.flag-wrapper .flag{position:relative;display:inline;background:#f8f8f8;padding:6px 10px;border-radius:5px;font-weight:600;text-align:left}@media (max-width: 860px){.flag-wrapper .flag{background:#fff;z-index:15}}.direction-l .flag:before,.direction-r .flag:before{position:absolute;top:60%;right:-40px;content:' ';display:block;width:12px;height:12px;margin-top:-10px;background:#fff;border-radius:10px;border:4px solid var(--link-color);z-index:10}@media (max-width: 860px){.direction-l .flag:before,.direction-r .flag:before{position:absolute;top:-30px;left:53%;content:' ';display:block;margin-left:-10px}}.time-wrapper{display:inline;line-height:1em;font-size:0.66666em;color:var(--link-color);vertical-align:middle}@media (max-width: 860px){.time-wrapper{display:block;position:relative;margin:4px 0 0 0;z-index:14}}.time-wrapper .time{display:inline-block;padding:4px 6px;background:#f8f8f8}.desc{margin:1em 0.2em -3.5em 0;font-size:0.9em;line-height:1.5em}.desc a{color:var(--link-color);text-decoration:none}.desc a:hover{text-decoration:underline}@media (max-width: 860px){.desc{position:relative;margin:1em 1em 0 1em;padding:1em;background:var(--code-bg-color);box-shadow:0 0 1px rgba(0,0,0,0.2);z-index:15}}</style></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-3EP7KLCYVP"></script> <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-3EP7KLCYVP'); </script><body><main><section class="post"><div class="flex-row-between"> <a href="https://p0wex.github.io/Gesko/">« back</a> <button title="Change theme" id="theme-toggle" onclick="modeSwitcher()"><div></div></button></div><time datetime="2022-12-20T00:00:00+01:00">Dec 20, 2022 - 16 ' read</time><h1 class="title"> Understanding diffusion models</h1><span class="meta"> </span><h1 id="understanding-diffusion-models"> UNDERSTANDING DIFFUSION MODELS <a href="#understanding-diffusion-models">#</a></h1><ol><li>__DIFFUSION MODELS + DENOISING DIFFUSION MODEL EXPLAINED</li><li>__3 POPULAR SOTA DIFFUSION MODELS</li><li>__SOURCE CODE FOR IMPLEMENTATION</li><li>__CONCLUSION</li><li>__REFERENCES</li></ol><p>It is no news that Diffusion models are the best of the best Generative models that have surpassed GANS in Computer Vision and other Multi-Modal Learning tasks. These models have created significant feats in arts, generative NFT’s, Interior design, personalized avatar creations, 3d modelling, urban design etc.</p><p>This post is going to be a two part series where I first of all expose how Denoising Diffusion models work which is the foundation upon which all these SOTA models (GLIDE, DALLE.2, SD) are being built.</p><h3 id="diffusion-models"> DIFFUSION MODELS <a href="#diffusion-models">#</a></h3><p>DIffusion model are a class of state-of-the art deep generative models that have shown impressive results on various tasks ranging from multi-modal modelling Vision to NLP to waveform signal Processing even to 3d object generation (but not comparable to humans). These models have achieved very impressive quality and diversity of sample synthesis than other state-of-the art genrative models like GANS, VAEs, Normalizing Flows etc.</p><table><tbody><tr><td>Diffusion model are generative models meaning it is a model used to generate data similar to the data which it was trained on. A generative model attempts to learn the data distribution over $p(x</td><td>y)$ where $x$ is the image and $y$ the labels in order to generate novel samples. Onced trained, a generative model can generate novel samples from an approximation of $p(x</td><td>y)$, denoted $p_\theta({x</td><td>y})$</td></tr></tbody></table><h3 id="areas-where-diffusion-models-have-been-applied-to"> Areas Where Diffusion Models have been applied to <a href="#areas-where-diffusion-models-have-been-applied-to">#</a></h3><ol><li>Image Super Resolution</li><li>Image Inpainting</li><li>Image Outpainting</li><li>Image to Image generation</li><li>Semantic Segmentation</li><li>Point Cloud Completion and Generation</li><li>Text-to-Image Generation</li><li>Text-to-Audio Generation</li></ol><h2 id="state-of-the-art-diffusion-models-cherrypicked"> STATE OF THE ART DIFFUSION MODELS (cherrypicked) <a href="#state-of-the-art-diffusion-models-cherrypicked">#</a></h2><ol><li>DALLE 2</li><li>STABLE DIFFUSION</li></ol><h2 id="understanding-the-foundations-ddpms"> UNDERSTANDING THE FOUNDATIONS (DDPMs) <a href="#understanding-the-foundations-ddpms">#</a></h2><p>Originally introduced by Sohl-Dickstein et al. (2015) in his paper “Deep unsupervised learning using nonequilibrium thermodynamics” which was followed up by Ho et al. (2020) in his paper “Denoising diffusion probabilistic models” (DDPMs), as well every other paper that has built on it. It is a latent variable generative model inspired by non-equilibrum thermodynamics in physics where samples generated is made possible by denoising process.</p><ol><li>Denoising Diffusion models is a two step model consisting of a forward process also called noising process and a backward process also know called denoisng or reverse noising process.</li><li>It works by adding gaussian noise to input data which could be an image following a Markov Chain.</li><li>The forward process follows a Markov chain where a little bit of gaussian noise is added to the input image which progressively disturbs the data distribution untill it is completely destroyed or unrecognised from gaussian noise.</li><li>The backward or reverse process learns to restore the data to its original form.</li><li>Reversing or removing the noise means recovering the values of the pixels, so that the resulting image will be similar to the original image. The reverse diffusion process takes a noisy image and learns to generate a less noisy version of that image, this process will be repeated until noise is converted to data. <strong>OR</strong></li><li>In order to generate new data, standard gaussian noise is used to perform the denoising. This process will be repeated until noise is converted to data.</li><li>The recovering or noise reversal is parameterized because it uses a neural network. The task of the Neural Network is to predict the noise that was added in a given image.</li><li>The objective function of this model was simplified to a MSE Loss i.e given a noised image, the noise added is predicted and then this predicted noise is subtracted from the noise to get the real image. This is basically what is happenning when training a Diffusion Model, It is learning to denoise.</li><li>In DDPMs the forward process is fixed while the reverse process is what needs learning meaning we need to train only a single neural network.</li><li>The important components of a denoising diffusion mdoels include:<ol><li>UNET Architecture</li><li>positional emebedidng</li><li>noise scheduler</li><li>attention mechanism</li></ol></li></ol><h2 id="mathematical-explanation"> MATHEMATICAL EXPLANATION <a href="#mathematical-explanation">#</a></h2><h3 id="a-noising"> A. NOISING: <a href="#a-noising">#</a></h3><h4 id="defining-the-process-formally"> DEFINING THE PROCESS FORMALLY <a href="#defining-the-process-formally">#</a></h4><h6 id="forward-process"> FORWARD PROCESS: <a href="#forward-process">#</a></h6><p>The forward diffusion process starts from data (image) and generates this intermediate noisy images by simply adding noise one step at a time At every step, a normal distribution will be used to generate an image conditioned on the previous image.</p><table><tbody><tr><td>the normal distribution which is represented as $q(x_t</td><td>x_{t-1}) = \mathcal{N(x_t; \sqrt{1-\beta_t{x_{t-1}}\beta_t}I)}$ is going to take $x_{t-1}$ the prevoius step and generate $x_t$ the current step. It takes $x_0$ and it generates $x_1$</td></tr></tbody></table><p>A normal distribution over the current step $x_t$ where the mean is $(\mathcal{\sqrt{1-\beta_t}})$ times the image at the prevoius time step which is ${x_{t-1}}$ and ${\beta_t}I$ represents the variance scheduler which in the real sense is a very small positive scalar value $0.001$</p><p>This normal distribution, $\mathcal{N(x_t; {\sqrt{1-\beta_t}{x_{t-1}}, \beta_t}I)}$ takes the image at the previous step, rescales the pixel values in this image and then adds tiny bit of noise via the variance scheduler “per time step”</p><h6 id="joint-distribution"> JOINT DISTRIBUTION: <a href="#joint-distribution">#</a></h6><p>A joint distibution can also be defined for all the samples generated in the forward process starting from $x_1$ all the way to $x_T$. The joint distribution which is the samples conditioned on $x_0$ is the cumulateive product of the conditionals that are formed at each step as such $q(x_1,…,x_T|x_0)$ defines the joint distribution of all the samples that will be generated in the forward markov process \(q(x_1,...,x_T|x_0) = \prod^T_{t=1} {q(x_t|x_{t-1})}\)</p><h6 id="speed"> Speed? <a href="#speed">#</a></h6><p>Why can’t we use $x_0$ our input image to generate noisy samples at any time step say $x_{10}$. Simply put can’t we use $x_0$ to generate $x_{10}$ ?. We can do that by making ${\alpha}_t$ = $1-\beta_t$ , then $\bar{\alpha}_t$ which is the cumulative product of ${\alpha}_t$ now becomes \(\bar{\alpha}_t = \prod^{t}_{s=1}(1-\beta_s)\) In order to answer the speed question we can then rewrite the original formular as follows: $q(x_t|x_0) = \mathcal{N(x_t;\sqrt{\bar{\alpha_t}}x_0,(1-{\bar{\alpha_t}})I)}$ Using the reparameterization trick we can sample $x_t$ as follows $x_t$ = $\sqrt{\bar{\alpha}}\space x_0 + \sqrt{1-\bar{\alpha_t}}\space \epsilon$ where $\epsilon \sim{\mathcal{N(0,1)}}$ and ${1-\bar{\alpha_t}}$ is our noise schedule at any time step, as such given $x_0$ we can draw samples at any time step $t$.</p><p>It should also be noted that the forward diffusion process is defined such that as $(x_T\mid{x_0})$ approaches infinity it becomes indistinguishable from standard normal distribution $\mathcal{N({x_T;(0,1)})}$. ___</p><ol><li>The forward chain pertubs the data distribution by gradually adding Gaussian noise to the ground truth image with a pre-designed schedule until the data distribution converges to a given prior, i.e., a standard Gaussian Distribution – (Isometric Gaussian). \(q(x_1,...,x_T|x_0) = \prod^T_{t=1} {q(x_t|x_{t-1})} ---(1)\) \(q(x_t|x_{t-1}) = \mathcal{N(x_t; \sqrt{1-\beta_t{x_{t-1}}\beta_t}I)} ---(2)\) $q(x_t)$ is used to denote the distributions of latent variables $x_t$ in the forward process.</li></ol><p>The noising process defined in Eq.(2) allows us to sample an arbitrary step of the noised latents directly conditioned on the input $x_o$. Where $\alpha_t$ = $1-\beta_t$ and $\bar{\alpha_t}$ = $\prod^t_{s=0}$ $\bar{\alpha_s}$, we can wite the marginal as: \(q(x_t|x_0) = \mathcal{N(x_t;\sqrt{\bar{\alpha_t}}x_0,(1-{\bar{\alpha_t}})I)}\) \(x_t = \sqrt{\bar{\alpha_t}}x_0 + \sqrt{1-{\bar{\alpha_t}}}\epsilon\) When $\bar{\alpha}_t$ approximates 0, $x_T$ is practically indistinguisahble from pure Gaussian noise: $p(x_T)\approx$ $\mathcal{N}(x_T;0,1)$.</p><h3 id="b-denoising--what-is-means-to-reverse-the-noise"> B. DENOISING: What is Means to Reverse The Noise? <a href="#b-denoising--what-is-means-to-reverse-the-noise">#</a></h3><h4 id="denoising-defining-the-generative-model-by-denoising"> DENOISING: DEFINING THE GENERATIVE MODEL BY DENOISING <a href="#denoising-defining-the-generative-model-by-denoising">#</a></h4><p>In order to generate data from a diffusion model, we will start from pure noise which is a standard normal distribution with zero mean and unit variance and generates data by denoising one step at a time.</p><p>As such $p(x_T)$ = $\mathcal{N(x_T;(0,1))}$ is the distribution of data at the end of the forward diffusion process. the parametric denoising distribution can be defined as follows $p_\theta(x_{t-1}\mid{x_t})$ = $\mathcal{N({x_{t-1};{\mu}<em>\theta{(x_t,t)},{\Sigma</em>\theta({x_t,{t}}})})}$ apart from the sample $x_t$ at time $t$ the model also takes $t$ as input in order to account for the different noise levels at different time steps in the forward process noise schedule so that the model can learn to undo this individually</p><h4 id="joint-distribution-1"> Joint Distribution <a href="#joint-distribution-1">#</a></h4><p>The joint distribution can be written as It is the product of the base distibution $p{(x_T)}$ and the product of the conditionals which still follows a markov process</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	$x_0\Leftarrow \cdots  \Leftarrow \cdots  \Leftarrow x_{T-1}\cdots  \Leftarrow x_T$
</code></pre></div></div>\[p_\theta(x_{0:T})=p(x_T)\prod^T_{t=1}p_\theta(x_{t-1}|x_t)\]<p>The model is now tasked with learning the probability density of an earlier time step given the current timestep. During this process iteration is done from pure noise $x_T$ to $x_0$ our final image</p><p>Starting from sampled noise, the diffusion model performs $T$ denoising steps until a sharp image is formed.</p><p>The denosing process produces a series of intermediate images with decreasing levels of noise, denote as $x_T, x_{T-1},…x_0$,</p><p>Given only $x_T$ which is indistinguishable from gaussian noise we can get $x_0$ an output image.</p><p>The reverse process takes the completely noised image and learns to gradually revert the Markov chain of noise corruption to the ground truth. The reversed process is then written as follows: \(p_\theta({x_{t-1}|x_t}) = \mathcal{N}(x_{t-1};\mu(x_t,t, {\Sigma}_\theta(x_t,t))\) <em>__ In Denoising Diffusion Models, The Noise $\epsilon$ is what is Predicted and this is done by Optimizing the variational upper bound on the negative log-likelihood. __</em> \(\\E\ [-\log{p_{\theta}}(x_0)]&lt; \\ E_q[-\log\frac{p_\theta({x_{0:T}})}{q(x_{1:T}|x_0)}]--(7)\) \(\\ E_q[-\log{p{(x_T)}}-\sum_{t&gt;=1}{\log\frac{p_\theta({x_{0:T}})}{q(x_{1:T}|x_0)}} ]-- (8)\) \(= -L_{VLB}\) <em>__ Reparametrization have been appplied to Eq. (8), which results in the general objective below: \(E_{t\sim{\mathcal{U(0,T),x_0{\sim{q{(x_0),\epsilon\sim{\mathcal{N(0,1)}}}}}}}}[\lambda{(t)}||\epsilon-{\epsilon_\theta}(x_t,t)||^2] --(10)\) __</em> The neural network ${\epsilon_\theta}(x_t,t)$ predicts $\epsilon$ by minimizing the loss = ${||\epsilon-\epsilon_\theta}(x_t,t)||^2$ which is the $L_2$ Loss INTUITIVELY: Given a noised image, the noise added is predicted and then this predicted noise is subtracted from the noise to get the real image. This is basically what is happenning when training a Diffusion Model, It is learning to denoise.</p><hr /><hr /><p>Both the forward Diffusion processes $q(x_t|x_{t-1})$ and the backward or reconstruction process $q(x_{t-1}|x_t)$ are modelled as the products of Markov transition probabilities: \(q(x_{0:T}) = q(x_0)\prod_{t=1}^T{q(x_t|x_{t-1})}, p_\theta(x_{T:0}) = p(x_{T})\prod_{t=T}^1{p_{\theta}(x_{t-1}|x_t)},\) $q(x_0)$ is the real data distribution</p><h4 id="important"> IMPORTant <a href="#important">#</a></h4><p>Diffusion models are latent variable models Latent variables$:$ = $x_1,x_{2},x_3,x_4,\cdots x_T$<br /> Observed variables$:$ $x_0$</p><h4 id="training-a-denosing-diffusion-probabilistic-model"> TRAINING A DENOSING DIFFUSION PROBABILISTIC MODEL <a href="#training-a-denosing-diffusion-probabilistic-model">#</a></h4><p>The reverse step process is only tasked with learning the mean while its variance is set to a constant <strong>__</strong></p><h5 id="objective-function-of-a-ddpm"> OBJECTIVE FUNCTION OF A DDPM <a href="#objective-function-of-a-ddpm">#</a></h5><p>\(E_{x_0{\sim{q{(x_0),\epsilon\sim{\mathcal{N(0,1)t\sim{\mathcal{U(0,T),}}}}}}}}[||\epsilon-{\epsilon_\theta}(\sqrt{\bar{\alpha}}\space x_0 + \sqrt{1-\bar{\alpha_t}}\space \epsilon),t||^2]\) Where $x_t = \sqrt{\bar{\alpha}}\space x_0 + \sqrt{1-\bar{\alpha_t}}\space \epsilon)$</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                     $E_{x_0{\sim{q{(x_0),\epsilon\sim{\mathcal{N(0,1)t\sim{\mathcal{U(0,T),}}}}}}}}[||\epsilon-{\epsilon_\theta}(x_t,t)||^2]$
</code></pre></div></div><p>Our loss funtion finally looks like this$:$ $L_{simple}={E_{x_0,t,\epsilon}}[||\epsilon_-\epsilon_{\theta}(x_t,t)||^2]$ <strong>__</strong>__</p><h6 id="the-training-algorithm-looks-like-this-traindiffpng"> The Training Algorithm looks like this: ![[trainDIFF.PNG]] <a href="#the-training-algorithm-looks-like-this-traindiffpng">#</a></h6><h4 id="image-generation-or-samling-from-a-denosing-diffusion-probabilistic-model"> IMAGE GENERATION or SAMLING FROM A DENOSING DIFFUSION PROBABILISTIC MODEL <a href="#image-generation-or-samling-from-a-denosing-diffusion-probabilistic-model">#</a></h4><hr /><h6 id="the-sampling-algorithm-looks-like-this-samplediffpng"> The Sampling Algorithm looks like this: ![[sampleDIFF.PNG]] <a href="#the-sampling-algorithm-looks-like-this-samplediffpng">#</a></h6><h2 id="important-components-of-a-denoising-diffusion-model"> IMPORTANT COMPONENTS OF A DENOISING DIFFUSION MODEL <a href="#important-components-of-a-denoising-diffusion-model">#</a></h2><ol><li><p>UNET ARCHITECTURE: The UNET Architecture is a Convolutional Network originally developed for biomedical image segmentation. It has a U Shape and it does downsampling on one part and upsampling on the other side and all its operation are convolution based.</p></li><li><p>POSITIONAL EMBEDDING: Positional embedding as originally used in the Attention is all you need paper was utilized for train the neural network has shared parameters across time which means it can’t distinguish between various timesteps as such it needs to filter the noise across images with different noise intensities.</p><p>PE is added as a way of encoding positional information into the UNET Model to help distinguish the various noise intensities within the markov chain process</p><p>The positional embedding is added as additional information in the downsample, middle and upsample block of the UNET Model.</p><p>PE are wave frequencies used to capture positonal information at both odd $sin()$ and even $cosine()$ positions and this embeddings can be calculated as follows.</p><p>$PE(<em>{pos,2i}) = sin(pos\div{1000^{2i/d}})$ $PE(</em>{pos,2i+1}) = cos(pos\div{1000^{2i/d}})$</p></li><li><p>NOISE SCHEDULER: This is just a technique that makes possible an iterative addition of noise to an image of adding noise.</p></li><li><p>ATTENTION MECHANISM: It is a mechanism that makes a model selectively focus on the varying parts of the inputs. It assigns weights to the different positions which indicates the importance of the input sequence for generating outputs by calculating attention scores. Originally in the transformers it was used both in the encoder and decoder parts of the Model.</p></li></ol><h2 id="implementing-denoisng-diffusion-model-with-pytorch"> IMPLEMENTING DENOISNG DIFFUSION MODEL WITH PYTORCH <a href="#implementing-denoisng-diffusion-model-with-pytorch">#</a></h2><p>It should be noted that I will not run training because it is needless and basically useless to train this kind of model because it is not state of the art as compared to the other models briefly explained above except you have a gpu compute where you can run it for over 1000 epochs. In <strong>part 2</strong> of this article you will see a detailed handson code for finetuning stable diffusion using dreambooth.</p><pre><code class="language-jupyter">import torch
from torch import nn
from torch.nn import functional as F
import numpy as np
import matplotlib.pyplot as plt
import torchvision
from torchvision import transforms
from torch.utils.data import DataLoader
from tqdm.notebook import tqdm
from torch.utils.data import Subset
import copy
</code></pre><pre><code class="language-jupyter">def show_images(images, title=""):
    """Shows the provided images as sub-pictures in a square"""
    images = [np.clip(im.permute(1,2,0).numpy(),0,1) for im in images]

  

    # Defining number of rows and columns
    fig = plt.figure(figsize=(8, 8))
    rows = int(len(images) ** (1 / 2))
    cols = round(len(images) / rows) 

    # Populating figure with sub-plots
    idx = 0
    for r in range(rows):
        for c in range(cols):
            fig.add_subplot(rows, cols, idx + 1)

            if idx &lt; len(images):
                plt.imshow(images[idx])
                plt.axis('off')
                idx += 1

    fig.suptitle(title, fontsize=30)
    # Showing the figure
    plt.show()
</code></pre><pre><code class="language-jupyter">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
</code></pre><pre><code class="language-jupyter">def sinusoidal_embedding(n, d):
    # Returns the standard positional embedding
    embedding = torch.tensor([[i / 10_000 ** (2 * j / d) for j in range(d)] for i in range(n)])
    sin_mask = torch.arange(0, n, 2)
    embedding[sin_mask] = torch.sin(embedding[sin_mask])
    embedding[1 - sin_mask] = torch.cos(embedding[sin_mask])
    return embedding
</code></pre><pre><code class="language-jupyter">class double_conv(nn.Module):
    def __init__(self, in_ch, out_ch):
        super(double_conv, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )

  
    def forward(self, x):
        x = self.conv(x)
        return x


class down_layer(nn.Module):
    def __init__(self, in_ch, out_ch):
        super(down_layer, self).__init__()
        self.pool = nn.MaxPool2d(2, stride=2, padding=0)
        self.conv = double_conv(in_ch, out_ch)

    def forward(self, x):
        x = self.conv(self.pool(x))
        return x
</code></pre><pre><code class="language-jupyter">class up(nn.Module):
    def __init__(self, in_ch, out_ch):
        super(up, self).__init__()
        self.up_scale = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)
  
    def forward(self, x1, x2): # x1 (bs,out_ch,w1,h1) x2 (bs,in_ch,w2,h2)
        x2 = self.up_scale(x2) # (bs,out_ch,2*w2,2*h2)
        diffY = x1.size()[2] - x2.size()[2]
        diffX = x1.size()[3] - x2.size()[3]
        x2 = F.pad(x2, [diffX // 2, diffX - diffX // 2,
                        diffY // 2, diffY - diffY // 2]) # (bs,out_ch,w1,h1)
        x = torch.cat([x2, x1], dim=1) # (bs,2*out_ch,w1,h1)
        return x

class up_layer(nn.Module):
    def __init__(self, in_ch, out_ch): # !! 2*out_ch = in_ch !!
        super(up_layer, self).__init__()
        self.up = up(in_ch, out_ch)
        self.conv = double_conv(in_ch, out_ch)  

    def forward(self, x1, x2): # x1 (bs,out_ch,w1,h1) x2 (bs,in_ch,w2,h2)
        a = self.up(x1, x2) # (bs,2*out_ch,w1,h1)
        x = self.conv(a) # (bs,out_ch,w1,h1) because 2*out_ch = in_ch
        return x
</code></pre><pre><code class="language-jupyter">class UNet(nn.Module):
    def __init__(self, in_channels=1, n_steps=1000, time_emb_dim=100):
        super(UNet, self).__init__()
        self.conv1 = double_conv(in_channels, 64)
        self.down1 = down_layer(64, 128)
        self.down2 = down_layer(128, 256)
        self.down3 = down_layer(256, 512)
        self.down4 = down_layer(512, 1024)
        self.up1 = up_layer(1024, 512)
        self.up2 = up_layer(512, 256)
        self.up3 = up_layer(256, 128)
        self.up4 = up_layer(128, 64)
        self.last_conv = nn.Conv2d(64, in_channels, 1)

        # Time embedding
        self.time_embed = nn.Embedding(n_steps, time_emb_dim)

        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)

        self.time_embed.requires_grad_(False)
        self.te1 = self._make_te(time_emb_dim, in_channels)
        self.te2 = self._make_te(time_emb_dim, 64)
        self.te3 = self._make_te(time_emb_dim, 128)
        self.te4 = self._make_te(time_emb_dim, 256)
        self.te5 = self._make_te(time_emb_dim, 512)
        self.te1_up = self._make_te(time_emb_dim, 1024)
        self.te2_up = self._make_te(time_emb_dim, 512)
        self.te3_up = self._make_te(time_emb_dim, 256)
        self.te4_up = self._make_te(time_emb_dim, 128)
  

    def _make_te(self, dim_in, dim_out):
        return nn.Sequential(nn.Linear(dim_in, dim_out), nn.SiLU(), nn.Linear(dim_out, dim_out))

    def forward(self, x , t): # x (bs,in_channels,w,d)
        bs = x.shape[0]
        t = self.time_embed(t)
        x1 = self.conv1(x+self.te1(t).reshape(bs, -1, 1, 1)) # (bs,64,w,d)
        x2 = self.down1(x1+self.te2(t).reshape(bs, -1, 1, 1)) # (bs,128,w/2,d/2)
        x3 = self.down2(x2+self.te3(t).reshape(bs, -1, 1, 1)) # (bs,256,w/4,d/4)
        x4 = self.down3(x3+self.te4(t).reshape(bs, -1, 1, 1)) # (bs,512,w/8,h/8)
        x5 = self.down4(x4+self.te5(t).reshape(bs, -1, 1, 1)) # (bs,1024,w/16,h/16)
        x1_up = self.up1(x4, x5+self.te1_up(t).reshape(bs, -1, 1, 1)) # (bs,512,w/8,h/8)
        x2_up = self.up2(x3, x1_up+self.te2_up(t).reshape(bs, -1, 1, 1)) # (bs,256,w/4,h/4)
        x3_up = self.up3(x2, x2_up+self.te3_up(t).reshape(bs, -1, 1, 1)) # (bs,128,w/2,h/2)
        x4_up = self.up4(x1, x3_up+self.te4_up(t).reshape(bs, -1, 1, 1)) # (bs,64,w,h)
        output = self.last_conv(x4_up) # (bs,in_channels,w,h)
        return output
</code></pre><pre><code class="language-jupyter">bs = 3
x = torch.randn(bs,1,32,32)
n_steps=1000
timesteps = torch.randint(0, n_steps, (bs,)).long()
unet = UNet()
</code></pre><pre><code class="language-jupyter">y = unet(x,timesteps)
y.shape
</code></pre><pre><code class="language-jupyter">class DDPM(nn.Module):
    def __init__(self, network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device) -&gt; None:
        super(DDPM, self).__init__()
        self.num_timesteps = num_timesteps
        self.betas = torch.linspace(beta_start, beta_end, num_timesteps, dtype=torch.float32).to(device)
        self.alphas = 1.0 - self.betas
        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)
        self.network = network
        self.device = device
        self.sqrt_alphas_cumprod = self.alphas_cumprod ** 0.5 # used in add_noise
        self.sqrt_one_minus_alphas_cumprod = (1 - self.alphas_cumprod) ** 0.5 # used in add_noise and step

  

    def add_noise(self, x_start, x_noise, timesteps):
        # The forward process
        # x_start and x_noise (bs, n_c, w, d)
        # timesteps (bs)
        s1 = self.sqrt_alphas_cumprod[timesteps] # bs
        s2 = self.sqrt_one_minus_alphas_cumprod[timesteps] # bs
        s1 = s1.reshape(-1,1,1,1) # (bs, 1, 1, 1) for broadcasting
        s2 = s2.reshape(-1,1,1,1) # (bs, 1, 1, 1)
        return s1 * x_start + s2 * x_noise

    def reverse(self, x, t):
        # The network return the estimation of the noise we added
        return self.network(x, t)
        
    def step(self, model_output, timestep, sample):
        # one step of sampling
        # timestep (1)
        t = timestep
        coef_epsilon = (1-self.alphas)/self.sqrt_one_minus_alphas_cumprod
        coef_eps_t = coef_epsilon[t].reshape(-1,1,1,1)
        coef_first = 1/self.alphas ** 0.5
        coef_first_t = coef_first[t].reshape(-1,1,1,1)
        pred_prev_sample = coef_first_t*(sample-coef_eps_t*model_output)

        variance = 0
        if t &gt; 0:
            noise = torch.randn_like(model_output).to(self.device)
            variance = ((self.betas[t] ** 0.5) * noise)
        pred_prev_sample = pred_prev_sample + variance
        return pred_prev_sample
</code></pre><pre><code class="language-jupyter">def training_loop(model, dataloader, optimizer, num_epochs, num_timesteps, device=device):
    """Training loop for DDPM"""
    global_step = 0
    losses = []
    for epoch in range(num_epochs):
        model.train()
        progress_bar = tqdm(total=len(dataloader))
        progress_bar.set_description(f"Epoch {epoch}")
        for step, batch in enumerate(dataloader):
            batch = batch[0].to(device)
            noise = torch.randn(batch.shape).to(device)
            timesteps = torch.randint(0, num_timesteps, (batch.shape[0],)).long().to(device)

  

            noisy = model.add_noise(batch, noise, timesteps)
            noise_pred = model.reverse(noisy, timesteps)
            loss = F.mse_loss(noise_pred, noise)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            progress_bar.update(1)
            logs = {"loss": loss.detach().item(), "step": global_step}
            losses.append(loss.detach().item())
            progress_bar.set_postfix(**logs)
            global_step += 1

        progress_bar.close()
</code></pre><pre><code class="language-jupyter">root_dir = './'

transforms01 = torchvision.transforms.Compose([
        torchvision.transforms.ToTensor(),
        #torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

dataset = torchvision.datasets.CIFAR10(root=root_dir, train=True, transform=transforms01, download=True)

dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=512, shuffle=True,num_workers=10)
</code></pre><pre><code class="language-jupyter">for b in dataloader:
    batch = b[0]
    break
    
bn = [b for b in batch[:100]]
show_images(bn, "origin")
</code></pre><p>![[res.png]]</p><pre><code class="language-jupyter">learning_rate = 1e-3
num_epochs = 15
num_timesteps = 1000
network = UNet(in_channels=3)
network.to(device)
model = DDPM(network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device)
model.train()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
training_loop(model, dataloader, optimizer, num_epochs, num_timesteps, device=device)
</code></pre><p>Model will start training for about 15 epochs</p><pre><code class="language-jupyter">def generate_image(ddpm, sample_size, channel, size):
    """Generate the image from the Gaussian noise"""
  
    frames = []
    frames_mid = []
    ddpm.eval()
    with torch.no_grad():
        timesteps = list(range(ddpm.num_timesteps))[::-1]
        sample = torch.randn(sample_size, channel, size, size).to(device)
        for i, t in enumerate(tqdm(timesteps)):
            time_tensor = (torch.ones(sample_size) * t).long().to(device)
            residual = ddpm.reverse(sample, time_tensor).to(device)

            sample = ddpm.step(residual, time_tensor[0], sample)

            if t==500:
                #sample_squeezed = torch.squeeze(sample)
                for i in range(sample_size):
                    frames_mid.append(sample[i].detach().cpu())

        #sample = torch.squeeze(sample)
        for i in range(sample_size):
            frames.append(sample[i].detach().cpu())
    return frames, frames_mid
</code></pre><pre><code class="language-jupyter">def make_dataloader(dataset, class_name ='ship'):
    s_indices = []
    s_idx = dataset.class_to_idx[class_name]
    for i in range(len(dataset)):
        current_class = dataset[i][1]
        if current_class == s_idx:
            s_indices.append(i)
    s_dataset = Subset(dataset, s_indices)
    return torch.utils.data.DataLoader(dataset=s_dataset, batch_size=512, shuffle=True)
</code></pre><pre><code class="language-jupyter">ship_dataloader = make_dataloader(dataset)
</code></pre><pre><code class="language-jupyter">ship_network = copy.deepcopy(network)
ship_model = DDPM(ship_network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device)
num_epochs = 10
num_timesteps = model.num_timesteps
learning_rate = 1e-3
ship_model.train()
optimizer = torch.optim.Adam(ship_model.parameters(), lr=learning_rate)
training_loop(ship_model, ship_dataloader, optimizer, num_epochs, num_timesteps, device=device)
</code></pre><pre><code class="language-jupyter">generated, generated_mid = generate_image(ship_model, 100, 3, 32)
</code></pre><pre><code class="language-jupyter">show_images(generated, "Generated ships")
</code></pre><p>![[ddpm.png]]</p><p>This training was only done for very few epochs thats why we still do not have a very detailed generated result.</p><h2 id="conclusions"> CONCLUSIONS <a href="#conclusions">#</a></h2><p>This article covered the foundations of Diffusion model by going deeply into Denoising diffusion models, the concepts, the maths and the code. There’s actually no point training this kind of an AI model because it wont result in any generation that is as good as DALL-E or Stable Diffusion in terms of Fidelity and Diveristy. As such we have SOTA Models that can perform text to image generation better than this one even though the purpose of this article was just to give an overview of the building blocks of a diffusion model. <em>__</em></p><h2 id="references"> REFERENCES <a href="#references">#</a></h2><ol><li>Karsten Kreis; Ruigui Gao; Arash Vahdat (2022-5-4): “Denoising Diffusion-based Generative Modelling: Foundations and Applications (CVPR 2002 Worskshop) “</li><li>Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models, 2020.</li><li>Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics, 2015.</li><li>https://github.com/dataflowr/notebooks/blob/master/Module18/ddpm_micro_sol.ipynb</li><li>Karsten Kreis; Ruigui Gao; Arash Vahdat (2022-5-4): “Denoising Diffusion-based Generative Modelling: Foundations and Applications (CVPR 2002 Worskshop) “</li></ol></section><section><nav class="post-nav"> <a class="post-nav-item post-nav-prev" href="/Gesko/2019/08/31/difference-between-font-formats"><div class="nav-arrow">Previous</div><span class="post-title">Difference between font formats</span> </a></nav></section><div id="disqus_thread" class="article-comments"></div><script> (function() { var d = document, s = d.createElement('script'); s.src = '//powex.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></main><script> function setTheme(theme) { if (theme === "dark") { document.documentElement.setAttribute('data-theme', 'dark'); window.localStorage.setItem('theme', 'dark'); document.getElementById("theme-toggle").classList.add('sun'); document.getElementById("theme-toggle").classList.remove('moon'); } else { document.documentElement.setAttribute('data-theme', 'light'); window.localStorage.setItem('theme', 'light'); document.getElementById("theme-toggle").classList.add('moon'); document.getElementById("theme-toggle").classList.remove('sun'); } } let theme = localStorage.getItem('theme'); theme = theme || (window.matchMedia ? window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light' : 'light'); setTheme(theme); function modeSwitcher() { let currentMode = document.documentElement.getAttribute('data-theme'); if (currentMode === "dark") { setTheme('light'); } else { setTheme('dark'); } } </script></body><footer class="social-footer"><div class="social-footer-icons"> <a href="https://github.com/P0WEX" title="Github profile, nothing much there, yet" target="_blank" rel="noopener noreferrer"><i class="fa fa-github" aria-hidden="true"></i></a> <a href="https://p0wex.github.io/Gesko/atom.xml" title="Feed RSS" target="_blank" rel="noopener noreferrer"><i class="fa fa-rss" aria-hidden="true"></i></a> <a href="https://github.com/P0WEX/Gesko" title="Browse the code" target="_blank" rel="noopener noreferrer"><i class="fa fa-code" aria-hidden="true"></i></a></div></footer></html>
